{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d2b0d4",
   "metadata": {},
   "source": [
    "## Metrics & formulas for batched LLM inference  \n",
    "\n",
    "### Notation\n",
    "\n",
    "| symbol | description |\n",
    "|--------|-------------|\n",
    "| $N$ | batch size (e.g. 100 prompts) |\n",
    "| $i$ | index of a sample in the batch |\n",
    "| $T_{\\text{batch}}$ | wall-clock time to generate the **whole** batch (measured once) |\n",
    "| $\\text{tok}_i$ | number of generated **tokens** in sample $i$ |\n",
    "| $\\displaystyle\\Sigma_{\\text{tok}}=\\sum_{j=1}^{N}\\text{tok}_j$ | total tokens in the batch |\n",
    "| $\\text{sent}_i$ | number of **sentences** in sample $i$ |\n",
    "| $\\displaystyle\\Sigma_{\\text{sent}}=\\sum_{j=1}^{N}\\text{sent}_j$ | total sentences in the batch |\n",
    "\n",
    "---\n",
    "\n",
    "### 1&nbsp;· Average-token latency (ATL)\n",
    "\n",
    "$$\n",
    "\\operatorname{ATL}_{\\text{batch}}\n",
    "    =\\frac{T_{\\text{batch}}}{\\Sigma_{\\text{tok}}}\n",
    "    \\quad\\text{[seconds / token]}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2&nbsp;· Per-sample generation latency (GL)\n",
    "\n",
    "$$\n",
    "\\operatorname{GL}_i\n",
    "    =\\operatorname{ATL}_{\\text{batch}}\\;\\text{tok}_i\n",
    "    =\\frac{\\text{tok}_i}{\\Sigma_{\\text{tok}}}\\;T_{\\text{batch}}\n",
    "    \\quad\\text{[seconds / sample]}\n",
    "$$\n",
    "\n",
    "Check:  $\\displaystyle\\sum_{i=1}^{N}\\operatorname{GL}_i=T_{\\text{batch}}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3 · Tokens per second (TPS)\n",
    "\n",
    "Batch-level throughput:\n",
    "\n",
    "$$\n",
    "\\text{TPS}_{\\text{batch}}\n",
    "  = \\frac{1}{\\text{ATL}_{\\text{batch}}}\n",
    "  = \\frac{\\Sigma_{\\text{tok}}}{T_{\\text{batch}}}\n",
    "$$\n",
    "\n",
    "*(units: tokens s⁻¹)*\n",
    "\n",
    "\n",
    "### 4 · Sentences per second (SPS)\n",
    "\n",
    "**Per-sample (row-level)**  \n",
    "\n",
    "$$\n",
    "\\text{SPS}_i\n",
    "  = \\frac{\\text{sent}_i}{\\text{GL}_i}\n",
    "  = \\frac{\\text{sent}_i}\n",
    "         {\\text{ATL}_{\\text{batch}}\\;\\text{tok}_i}\n",
    "$$\n",
    "\n",
    "**Batch-level (one value per batch)**  \n",
    "\n",
    "$$\n",
    "\\text{SPS}_{\\text{batch}}\n",
    "  = \\frac{\\Sigma_{\\text{sent}}}{T_{\\text{batch}}}\n",
    "  = \\frac{1}{N} \\sum_{i=1}^{N} \\text{SPS}_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5&nbsp;· Consistency checks (should hold after the fix)\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N}\\operatorname{GL}_i = T_{\\text{batch}},\\qquad\n",
    "\\frac{1}{N}\\sum_{i=1}^{N}\\operatorname{SPS}_i = \\operatorname{SPS}_{\\text{batch}},\\qquad\n",
    "\\operatorname{TPS}_{\\text{batch}}\\;T_{\\text{batch}} = \\Sigma_{\\text{tok}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39792c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"/home/ubuntu/fast_llm_inference/results/experiment_1/vllm_gemma-2-2b-it_qa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec92eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, argparse, pathlib\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "# ─── lightweight token / sentence counters ────────────────────────────\n",
    "_tok_re  = re.compile(r\"\\S+\")\n",
    "_sent_re = re.compile(r\"[.!?…]+\")\n",
    "\n",
    "def _tok_cnt(text: str) -> int:\n",
    "    return len(_tok_re.findall(text))\n",
    "\n",
    "def _sent_cnt(text: str) -> int:\n",
    "    return max(len(_sent_re.findall(text)), 1)\n",
    "\n",
    "# ─── main fixer ────────────────────────────────────────────────────────\n",
    "def fix_metrics(df: pd.DataFrame, batch_size: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fixes latency/throughput and overrides energy-per-unit columns in-place.\n",
    "    Expects:\n",
    "      - 'generated_answer'    : str\n",
    "      - 'GL'                  : float (batch latency duplicated on every row)\n",
    "      - 'Total Energy (Wh)'   : float (duplicated per row)\n",
    "      - 'Energy per Token (J/token)'       : float (will be replaced)\n",
    "      - 'Energy per Sentence (J/sentence)' : float (will be replaced)\n",
    "    Returns the DataFrame with corrected:\n",
    "      ATL, GL, TPS, SPS,\n",
    "      Energy per Token (J/token),\n",
    "      Energy per Sentence (J/sentence)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) derive token & sentence counts if missing -----------------------\n",
    "    if \"num_tokens\" not in df.columns:\n",
    "        df[\"num_tokens\"] = df[\"generated_answer\"].map(_tok_cnt)\n",
    "    if \"num_sentences\" not in df.columns:\n",
    "        df[\"num_sentences\"] = df[\"generated_answer\"].map(_sent_cnt)\n",
    "\n",
    "    # 2) tag rows by batch -----------------------------------------------\n",
    "    df[\"batch_id\"] = (df.index // batch_size).astype(int)\n",
    "\n",
    "    # 3) compute per-batch scalars ---------------------------------------\n",
    "    df[\"batch_time_s\"]      = df.groupby(\"batch_id\")[\"GL\"].transform(\"first\")\n",
    "    df[\"batch_tokens\"]      = df.groupby(\"batch_id\")[\"num_tokens\"].transform(\"sum\")\n",
    "    df[\"batch_sentences\"]   = df.groupby(\"batch_id\")[\"num_sentences\"].transform(\"sum\")\n",
    "    batch_energy_wh         = df.groupby(\"batch_id\")[\"Total Energy (Wh)\"].transform(\"first\")\n",
    "\n",
    "    # 4) correct latency & throughput ------------------------------------\n",
    "    df[\"ATL\"] = df[\"batch_time_s\"] / df[\"batch_tokens\"]      # seconds / token\n",
    "    df[\"GL\"]  = df[\"ATL\"] * df[\"num_tokens\"]                 # seconds / sample\n",
    "    df[\"TPS\"] = 1.0 / df[\"ATL\"]                              # tokens / second\n",
    "    df[\"SPS\"] = df[\"num_sentences\"] / df[\"GL\"]               # sentences / second\n",
    "\n",
    "    # 5) override energy-per-unit columns in-place -----------------------\n",
    "    # convert Wh → J by multiplying by 3600\n",
    "    df[\"Energy per Token (J/token)\"]     = batch_energy_wh * 3600 / df[\"batch_tokens\"]\n",
    "    df[\"Energy per Sentence (J/sentence)\"] = batch_energy_wh * 3600 / df[\"batch_sentences\"]\n",
    "\n",
    "    # 6) drop internal helpers -------------------------------------------\n",
    "    df.drop(columns=[\"batch_id\", \"batch_time_s\", \"batch_tokens\", \"batch_sentences\", \"num_tokens\", \"num_sentences\"], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe8705e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>TTFT</th>\n",
       "      <th>ATL</th>\n",
       "      <th>GL</th>\n",
       "      <th>TPS</th>\n",
       "      <th>SPS</th>\n",
       "      <th>Avg GPU Mem (MB)</th>\n",
       "      <th>...</th>\n",
       "      <th>Avg GPU Util (%)</th>\n",
       "      <th>Total Energy (Wh)</th>\n",
       "      <th>Avg Power (W)</th>\n",
       "      <th>Energy per Token (J/token)</th>\n",
       "      <th>Energy per Sentence (J/sentence)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>Overhead (MB)</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1275</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>Lothar de Maizière</td>\n",
       "      <td>['Lothar de Maizière', 'Lothar de Maizière', '...</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>0.029107</td>\n",
       "      <td>103.069379</td>\n",
       "      <td>34.356460</td>\n",
       "      <td>21351.79</td>\n",
       "      <td>...</td>\n",
       "      <td>79.73</td>\n",
       "      <td>0.034697</td>\n",
       "      <td>56.71</td>\n",
       "      <td>0.550261</td>\n",
       "      <td>1.249092</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>5007.298138</td>\n",
       "      <td>16346.581862</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>753</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>Complexity classes</td>\n",
       "      <td>['complexity classes', 'complexity classes', '...</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>0.019404</td>\n",
       "      <td>103.069379</td>\n",
       "      <td>51.534689</td>\n",
       "      <td>21351.79</td>\n",
       "      <td>...</td>\n",
       "      <td>79.73</td>\n",
       "      <td>0.034697</td>\n",
       "      <td>56.71</td>\n",
       "      <td>0.550261</td>\n",
       "      <td>1.249092</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>5007.298138</td>\n",
       "      <td>16346.581862</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1197</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>GTE</td>\n",
       "      <td>['Telenet was incorporated in 1973 and started...</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>103.069379</td>\n",
       "      <td>103.069379</td>\n",
       "      <td>21351.79</td>\n",
       "      <td>...</td>\n",
       "      <td>79.73</td>\n",
       "      <td>0.034697</td>\n",
       "      <td>56.71</td>\n",
       "      <td>0.550261</td>\n",
       "      <td>1.249092</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>5007.298138</td>\n",
       "      <td>16346.581862</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1541</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>Water flow</td>\n",
       "      <td>['water flow through the body cavity', \"κτείς ...</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>0.019404</td>\n",
       "      <td>103.069379</td>\n",
       "      <td>51.534689</td>\n",
       "      <td>21351.79</td>\n",
       "      <td>...</td>\n",
       "      <td>79.73</td>\n",
       "      <td>0.034697</td>\n",
       "      <td>56.71</td>\n",
       "      <td>0.550261</td>\n",
       "      <td>1.249092</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>5007.298138</td>\n",
       "      <td>16346.581862</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1747</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>1705</td>\n",
       "      <td>['12 May 1705', '1705', '12 May 1705']</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>103.069379</td>\n",
       "      <td>103.069379</td>\n",
       "      <td>21351.79</td>\n",
       "      <td>...</td>\n",
       "      <td>79.73</td>\n",
       "      <td>0.034697</td>\n",
       "      <td>56.71</td>\n",
       "      <td>0.550261</td>\n",
       "      <td>1.249092</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>5007.298138</td>\n",
       "      <td>16346.581862</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1157</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>1,600</td>\n",
       "      <td>['1,600 miles', '1,600', '1,600']</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>109.650142</td>\n",
       "      <td>109.650142</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>...</td>\n",
       "      <td>90.45</td>\n",
       "      <td>0.040963</td>\n",
       "      <td>68.52</td>\n",
       "      <td>0.624859</td>\n",
       "      <td>1.474668</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>5007.298138</td>\n",
       "      <td>16346.581862</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2207</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>Krasiński Palace Garden</td>\n",
       "      <td>['Krasiński Palace Garden', 'Krasiński Palace ...</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>0.027360</td>\n",
       "      <td>109.650142</td>\n",
       "      <td>36.550047</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>...</td>\n",
       "      <td>90.45</td>\n",
       "      <td>0.040963</td>\n",
       "      <td>68.52</td>\n",
       "      <td>0.624859</td>\n",
       "      <td>1.474668</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>5007.298138</td>\n",
       "      <td>16346.581862</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1115</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>214</td>\n",
       "      <td>['489', '489', '489']</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>109.650142</td>\n",
       "      <td>109.650142</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>...</td>\n",
       "      <td>90.45</td>\n",
       "      <td>0.040963</td>\n",
       "      <td>68.52</td>\n",
       "      <td>0.624859</td>\n",
       "      <td>1.474668</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>5007.298138</td>\n",
       "      <td>16346.581862</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1830</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>Mnemiopsis leidyi</td>\n",
       "      <td>['ctenophore Mnemiopsis leidyi', 'Mnemiopsis l...</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>0.018240</td>\n",
       "      <td>109.650142</td>\n",
       "      <td>54.825071</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>...</td>\n",
       "      <td>90.45</td>\n",
       "      <td>0.040963</td>\n",
       "      <td>68.52</td>\n",
       "      <td>0.624859</td>\n",
       "      <td>1.474668</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>5007.298138</td>\n",
       "      <td>16346.581862</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1093</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>November 28, 1995</td>\n",
       "      <td>['November 28, 1995', 'November 28, 1995', 'No...</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>0.027360</td>\n",
       "      <td>109.650142</td>\n",
       "      <td>36.550047</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>...</td>\n",
       "      <td>90.45</td>\n",
       "      <td>0.040963</td>\n",
       "      <td>68.52</td>\n",
       "      <td>0.624859</td>\n",
       "      <td>1.474668</td>\n",
       "      <td>21353.88</td>\n",
       "      <td>5007.298138</td>\n",
       "      <td>16346.581862</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_length                                             prompt  \\\n",
       "0             1275  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "1              753  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "2             1197  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "3             1541  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "4             1747  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "..             ...                                                ...   \n",
       "495           1157  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "496           2207  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "497           1115  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "498           1830  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "499           1093  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "\n",
       "            generated_answer  \\\n",
       "0         Lothar de Maizière   \n",
       "1         Complexity classes   \n",
       "2                        GTE   \n",
       "3                 Water flow   \n",
       "4                       1705   \n",
       "..                       ...   \n",
       "495                    1,600   \n",
       "496  Krasiński Palace Garden   \n",
       "497                      214   \n",
       "498        Mnemiopsis leidyi   \n",
       "499        November 28, 1995   \n",
       "\n",
       "                                      reference_answer    TTFT       ATL  \\\n",
       "0    ['Lothar de Maizière', 'Lothar de Maizière', '...  0.0283  0.009702   \n",
       "1    ['complexity classes', 'complexity classes', '...  0.0283  0.009702   \n",
       "2    ['Telenet was incorporated in 1973 and started...  0.0283  0.009702   \n",
       "3    ['water flow through the body cavity', \"κτείς ...  0.0283  0.009702   \n",
       "4               ['12 May 1705', '1705', '12 May 1705']  0.0283  0.009702   \n",
       "..                                                 ...     ...       ...   \n",
       "495                  ['1,600 miles', '1,600', '1,600']  0.0276  0.009120   \n",
       "496  ['Krasiński Palace Garden', 'Krasiński Palace ...  0.0276  0.009120   \n",
       "497                              ['489', '489', '489']  0.0276  0.009120   \n",
       "498  ['ctenophore Mnemiopsis leidyi', 'Mnemiopsis l...  0.0276  0.009120   \n",
       "499  ['November 28, 1995', 'November 28, 1995', 'No...  0.0276  0.009120   \n",
       "\n",
       "           GL         TPS         SPS  Avg GPU Mem (MB)  ...  \\\n",
       "0    0.029107  103.069379   34.356460          21351.79  ...   \n",
       "1    0.019404  103.069379   51.534689          21351.79  ...   \n",
       "2    0.009702  103.069379  103.069379          21351.79  ...   \n",
       "3    0.019404  103.069379   51.534689          21351.79  ...   \n",
       "4    0.009702  103.069379  103.069379          21351.79  ...   \n",
       "..        ...         ...         ...               ...  ...   \n",
       "495  0.009120  109.650142  109.650142          21353.88  ...   \n",
       "496  0.027360  109.650142   36.550047          21353.88  ...   \n",
       "497  0.009120  109.650142  109.650142          21353.88  ...   \n",
       "498  0.018240  109.650142   54.825071          21353.88  ...   \n",
       "499  0.027360  109.650142   36.550047          21353.88  ...   \n",
       "\n",
       "     Avg GPU Util (%)  Total Energy (Wh)  Avg Power (W)  \\\n",
       "0               79.73           0.034697          56.71   \n",
       "1               79.73           0.034697          56.71   \n",
       "2               79.73           0.034697          56.71   \n",
       "3               79.73           0.034697          56.71   \n",
       "4               79.73           0.034697          56.71   \n",
       "..                ...                ...            ...   \n",
       "495             90.45           0.040963          68.52   \n",
       "496             90.45           0.040963          68.52   \n",
       "497             90.45           0.040963          68.52   \n",
       "498             90.45           0.040963          68.52   \n",
       "499             90.45           0.040963          68.52   \n",
       "\n",
       "     Energy per Token (J/token)  Energy per Sentence (J/sentence)  \\\n",
       "0                      0.550261                          1.249092   \n",
       "1                      0.550261                          1.249092   \n",
       "2                      0.550261                          1.249092   \n",
       "3                      0.550261                          1.249092   \n",
       "4                      0.550261                          1.249092   \n",
       "..                          ...                               ...   \n",
       "495                    0.624859                          1.474668   \n",
       "496                    0.624859                          1.474668   \n",
       "497                    0.624859                          1.474668   \n",
       "498                    0.624859                          1.474668   \n",
       "499                    0.624859                          1.474668   \n",
       "\n",
       "     Memory Usage (MB)  Model Size (MB)  Overhead (MB)  exact_match  F1_score  \n",
       "0             21353.88      5007.298138   16346.581862            1  1.000000  \n",
       "1             21353.88      5007.298138   16346.581862            1  1.000000  \n",
       "2             21353.88      5007.298138   16346.581862            1  1.000000  \n",
       "3             21353.88      5007.298138   16346.581862            0  0.571429  \n",
       "4             21353.88      5007.298138   16346.581862            1  1.000000  \n",
       "..                 ...              ...            ...          ...       ...  \n",
       "495           21353.88      5007.298138   16346.581862            1  1.000000  \n",
       "496           21353.88      5007.298138   16346.581862            1  1.000000  \n",
       "497           21353.88      5007.298138   16346.581862            0  0.000000  \n",
       "498           21353.88      5007.298138   16346.581862            1  1.000000  \n",
       "499           21353.88      5007.298138   16346.581862            1  1.000000  \n",
       "\n",
       "[500 rows x 21 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_data = fix_metrics(data, batch_size=100)\n",
    "\n",
    "fixed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2008f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# merge_run_reports.py\n",
    "# Usage: python merge_run_reports.py /home/ubuntu/fast_llm_inference/RQ1_merged_256/run_report\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ─────────────────────────────────────── config\n",
    "RUN_DIR   = Path(\"/home/ubuntu/fast_llm_inference/RQ1_merged_256/run_report\")\n",
    "OUT_PATH  = RUN_DIR.parent / \"merged_run_report.csv\"\n",
    "\n",
    "# All columns we want in the final file (order matters)\n",
    "FINAL_COLS = [\n",
    "    \"model_name\", \"model_size_mb\", \"task\", \"scenario\", \"backend\",\n",
    "    \"startup\", \"ttft_sec\", \"coldstart\", \"batch_size\", \"num_queries\",\n",
    "    \"total_generation_time_s\", \"avg_gpu_mem_mb\", \"peak_gpu_mem_mb\",\n",
    "    \"overhead_mb\", \"avg_gpu_util_pct\", \"peak_gpu_util_pct\",\n",
    "    \"avg_cpu_util_pct\", \"peak_cpu_util_pct\", \"avg_ram_mb\", \"peak_ram_mb\",\n",
    "    \"avg_power_w\", \"peak_power_w\", \"total_energy_wh\",\n",
    "    \"avg_generation_time\", \"avg_tokens_generated\", \"avg_sentences_generated\",\n",
    "    \"avg_ATL\", \"avg_GL\", \"avg_TPS\", \"avg_SPS\",\n",
    "    \"avg_energy_per_token\", \"avg_energy_per_sentence\"\n",
    "]\n",
    "\n",
    "# Quality‐metric columns we need for pick_quality()\n",
    "QUALITY_COLS = [\"avg_F1_score\", \"avg_AST_equal\", \"avg_ROUGE-1\"]\n",
    "\n",
    "def load_one(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read one run_report CSV and normalize to FINAL_COLS + QUALITY_COLS schema.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Ensure every expected column exists; missing ⇒ NaN\n",
    "    for col in FINAL_COLS + QUALITY_COLS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # Keep only the final schema & order\n",
    "    return df[FINAL_COLS + QUALITY_COLS]\n",
    "\n",
    "# ─────────────────────────────────────── main\n",
    "all_csvs = sorted(RUN_DIR.glob(\"*.csv\"))\n",
    "if not all_csvs:\n",
    "    sys.exit(f\"No CSVs found in {RUN_DIR}\")\n",
    "\n",
    "# Load & normalize each, then concatenate\n",
    "merged = pd.concat([load_one(p) for p in all_csvs], ignore_index=True)\n",
    "\n",
    "# ── write out ───────────────────────────────────────────────────────\n",
    "merged.to_csv(OUT_PATH, index=False)\n",
    "print(f\"🚀  Merged {len(all_csvs)} files → {OUT_PATH}  ({len(merged)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e267bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/home/ubuntu/fast_llm_inference/results/llama.cpp_gemma-2-2b-it-fp16.gguf_sql_1QPS_120s_server.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54b24686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt_length', 'prompt', 'generated_answer', 'reference_answer',\n",
       "       'queue_size', 'batch_size', 'wait_time', 'response_time',\n",
       "       'scheduled_ts', 'start_ts', 'GL', 'ATL', 'TTFT', 'TPS', 'SPS',\n",
       "       'Avg GPU Mem (MB)', 'Peak GPU Mem (MB)', 'Avg GPU Util (%)',\n",
       "       'Peak GPU Util (%)', 'Total Energy (Wh)', 'Avg Power (W)',\n",
       "       'Peak Power (W)', 'Energy per Token (J/token)',\n",
       "       'Energy per Sentence (J/sentence)', 'Memory Usage (MB)',\n",
       "       'Model Size (MB)', 'Overhead (MB)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21447956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>queue_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>wait_time</th>\n",
       "      <th>response_time</th>\n",
       "      <th>scheduled_ts</th>\n",
       "      <th>start_ts</th>\n",
       "      <th>...</th>\n",
       "      <th>Avg GPU Util (%)</th>\n",
       "      <th>Peak GPU Util (%)</th>\n",
       "      <th>Total Energy (Wh)</th>\n",
       "      <th>Avg Power (W)</th>\n",
       "      <th>Peak Power (W)</th>\n",
       "      <th>Energy per Token (J/token)</th>\n",
       "      <th>Energy per Sentence (J/sentence)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>Overhead (MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>984</td>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT AirportCode FROM airports ORDER BY COUN...</td>\n",
       "      <td>SELECT T1.AirportCode FROM AIRPORTS AS T1 JOIN...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.4945</td>\n",
       "      <td>1.3042</td>\n",
       "      <td>1.3048</td>\n",
       "      <td>...</td>\n",
       "      <td>40.00</td>\n",
       "      <td>86</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>41.41</td>\n",
       "      <td>47.83</td>\n",
       "      <td>2.045035</td>\n",
       "      <td>20.450349</td>\n",
       "      <td>7005.88</td>\n",
       "      <td>4992.689056</td>\n",
       "      <td>2013.190944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>952</td>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT COUNT(*) FROM Student AS s JOIN Has_Pet...</td>\n",
       "      <td>SELECT count(*) FROM student AS T1 JOIN has_pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>1.5940</td>\n",
       "      <td>2.2943</td>\n",
       "      <td>2.3107</td>\n",
       "      <td>...</td>\n",
       "      <td>71.57</td>\n",
       "      <td>87</td>\n",
       "      <td>0.025997</td>\n",
       "      <td>59.33</td>\n",
       "      <td>73.20</td>\n",
       "      <td>2.836066</td>\n",
       "      <td>18.718035</td>\n",
       "      <td>7005.88</td>\n",
       "      <td>4992.689056</td>\n",
       "      <td>2013.190944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1984</td>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>```sql</td>\n",
       "      <td>SELECT avg(transcript_date) FROM Transcripts</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.3742</td>\n",
       "      <td>2.6147</td>\n",
       "      <td>2.5643</td>\n",
       "      <td>3.9385</td>\n",
       "      <td>...</td>\n",
       "      <td>78.10</td>\n",
       "      <td>91</td>\n",
       "      <td>0.023225</td>\n",
       "      <td>67.40</td>\n",
       "      <td>73.20</td>\n",
       "      <td>83.609322</td>\n",
       "      <td>83.609322</td>\n",
       "      <td>7009.88</td>\n",
       "      <td>4992.689056</td>\n",
       "      <td>2017.190944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1084</td>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>```sql</td>\n",
       "      <td>SELECT name FROM battle WHERE bulgarian_comman...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7772</td>\n",
       "      <td>2.0177</td>\n",
       "      <td>3.1613</td>\n",
       "      <td>3.9385</td>\n",
       "      <td>...</td>\n",
       "      <td>78.10</td>\n",
       "      <td>91</td>\n",
       "      <td>0.023225</td>\n",
       "      <td>67.40</td>\n",
       "      <td>73.20</td>\n",
       "      <td>83.609322</td>\n",
       "      <td>83.609322</td>\n",
       "      <td>7009.88</td>\n",
       "      <td>4992.689056</td>\n",
       "      <td>2017.190944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1385</td>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>```sql</td>\n",
       "      <td>SELECT first_name ,  last_name FROM players WH...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>1.5931</td>\n",
       "      <td>6.9972</td>\n",
       "      <td>7.0398</td>\n",
       "      <td>...</td>\n",
       "      <td>64.65</td>\n",
       "      <td>85</td>\n",
       "      <td>0.026734</td>\n",
       "      <td>62.07</td>\n",
       "      <td>72.98</td>\n",
       "      <td>96.243709</td>\n",
       "      <td>96.243709</td>\n",
       "      <td>7009.88</td>\n",
       "      <td>4992.689056</td>\n",
       "      <td>2017.190944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>934</td>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT PetType, AVG(weight) FROM Pets GROUP BY...</td>\n",
       "      <td>SELECT avg(weight) ,  pettype FROM pets GROUP ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>108.6052</td>\n",
       "      <td>108.6055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>38.49</td>\n",
       "      <td>40.44</td>\n",
       "      <td>2.178195</td>\n",
       "      <td>17.425557</td>\n",
       "      <td>7011.88</td>\n",
       "      <td>4992.689056</td>\n",
       "      <td>2019.190944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1198</td>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>```sql</td>\n",
       "      <td>SELECT T1.series_name FROM TV_Channel AS T1 JO...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>1.3120</td>\n",
       "      <td>113.5299</td>\n",
       "      <td>113.5666</td>\n",
       "      <td>...</td>\n",
       "      <td>67.91</td>\n",
       "      <td>86</td>\n",
       "      <td>0.020092</td>\n",
       "      <td>56.72</td>\n",
       "      <td>68.47</td>\n",
       "      <td>72.331712</td>\n",
       "      <td>72.331712</td>\n",
       "      <td>7011.88</td>\n",
       "      <td>4992.689056</td>\n",
       "      <td>2019.190944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1615</td>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>```sql</td>\n",
       "      <td>SELECT T1.date_of_treatment ,  T2.first_name F...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8646</td>\n",
       "      <td>1.9765</td>\n",
       "      <td>114.1040</td>\n",
       "      <td>114.9686</td>\n",
       "      <td>...</td>\n",
       "      <td>74.30</td>\n",
       "      <td>88</td>\n",
       "      <td>0.021860</td>\n",
       "      <td>70.77</td>\n",
       "      <td>71.83</td>\n",
       "      <td>78.697108</td>\n",
       "      <td>78.697108</td>\n",
       "      <td>7011.88</td>\n",
       "      <td>4992.689056</td>\n",
       "      <td>2019.190944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1190</td>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>```sql</td>\n",
       "      <td>select t1.id ,  t1.maker from car_makers as t1...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>2.2753</td>\n",
       "      <td>116.6142</td>\n",
       "      <td>116.6311</td>\n",
       "      <td>...</td>\n",
       "      <td>70.71</td>\n",
       "      <td>90</td>\n",
       "      <td>0.039308</td>\n",
       "      <td>62.66</td>\n",
       "      <td>73.00</td>\n",
       "      <td>141.509054</td>\n",
       "      <td>141.509054</td>\n",
       "      <td>7011.88</td>\n",
       "      <td>4992.689056</td>\n",
       "      <td>2019.190944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2005</td>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>```sql</td>\n",
       "      <td>SELECT DISTINCT T1.course_name FROM Courses AS...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>2.4021</td>\n",
       "      <td>118.1275</td>\n",
       "      <td>119.0050</td>\n",
       "      <td>...</td>\n",
       "      <td>73.55</td>\n",
       "      <td>91</td>\n",
       "      <td>0.028082</td>\n",
       "      <td>66.31</td>\n",
       "      <td>70.52</td>\n",
       "      <td>101.093517</td>\n",
       "      <td>101.093517</td>\n",
       "      <td>7011.88</td>\n",
       "      <td>4992.689056</td>\n",
       "      <td>2019.190944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_length                                             prompt  \\\n",
       "0              984  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "1              952  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "2             1984  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "3             1084  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "4             1385  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "..             ...                                                ...   \n",
       "98             934  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "99            1198  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "100           1615  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "101           1190  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "102           2005  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "\n",
       "                                      generated_answer  \\\n",
       "0    SELECT AirportCode FROM airports ORDER BY COUN...   \n",
       "1    SELECT COUNT(*) FROM Student AS s JOIN Has_Pet...   \n",
       "2                                               ```sql   \n",
       "3                                               ```sql   \n",
       "4                                               ```sql   \n",
       "..                                                 ...   \n",
       "98   SELECT PetType, AVG(weight) FROM Pets GROUP BY...   \n",
       "99                                              ```sql   \n",
       "100                                             ```sql   \n",
       "101                                             ```sql   \n",
       "102                                             ```sql   \n",
       "\n",
       "                                      reference_answer  queue_size  \\\n",
       "0    SELECT T1.AirportCode FROM AIRPORTS AS T1 JOIN...           1   \n",
       "1    SELECT count(*) FROM student AS T1 JOIN has_pe...           1   \n",
       "2         SELECT avg(transcript_date) FROM Transcripts           2   \n",
       "3    SELECT name FROM battle WHERE bulgarian_comman...           2   \n",
       "4    SELECT first_name ,  last_name FROM players WH...           1   \n",
       "..                                                 ...         ...   \n",
       "98   SELECT avg(weight) ,  pettype FROM pets GROUP ...           1   \n",
       "99   SELECT T1.series_name FROM TV_Channel AS T1 JO...           1   \n",
       "100  SELECT T1.date_of_treatment ,  T2.first_name F...           1   \n",
       "101  select t1.id ,  t1.maker from car_makers as t1...           1   \n",
       "102  SELECT DISTINCT T1.course_name FROM Courses AS...           1   \n",
       "\n",
       "     batch_size  wait_time  response_time  scheduled_ts  start_ts  ...  \\\n",
       "0             1     0.0007         0.4945        1.3042    1.3048  ...   \n",
       "1             1     0.0165         1.5940        2.2943    2.3107  ...   \n",
       "2             2     1.3742         2.6147        2.5643    3.9385  ...   \n",
       "3             2     0.7772         2.0177        3.1613    3.9385  ...   \n",
       "4             1     0.0426         1.5931        6.9972    7.0398  ...   \n",
       "..          ...        ...            ...           ...       ...  ...   \n",
       "98            1     0.0002         0.4529      108.6052  108.6055  ...   \n",
       "99            1     0.0367         1.3120      113.5299  113.5666  ...   \n",
       "100           1     0.8646         1.9765      114.1040  114.9686  ...   \n",
       "101           1     0.0169         2.2753      116.6142  116.6311  ...   \n",
       "102           1     0.8775         2.4021      118.1275  119.0050  ...   \n",
       "\n",
       "     Avg GPU Util (%)  Peak GPU Util (%)  Total Energy (Wh)  Avg Power (W)  \\\n",
       "0               40.00                 86           0.005681          41.41   \n",
       "1               71.57                 87           0.025997          59.33   \n",
       "2               78.10                 91           0.023225          67.40   \n",
       "3               78.10                 91           0.023225          67.40   \n",
       "4               64.65                 85           0.026734          62.07   \n",
       "..                ...                ...                ...            ...   \n",
       "98               0.00                  0           0.004840          38.49   \n",
       "99              67.91                 86           0.020092          56.72   \n",
       "100             74.30                 88           0.021860          70.77   \n",
       "101             70.71                 90           0.039308          62.66   \n",
       "102             73.55                 91           0.028082          66.31   \n",
       "\n",
       "     Peak Power (W)  Energy per Token (J/token)  \\\n",
       "0             47.83                    2.045035   \n",
       "1             73.20                    2.836066   \n",
       "2             73.20                   83.609322   \n",
       "3             73.20                   83.609322   \n",
       "4             72.98                   96.243709   \n",
       "..              ...                         ...   \n",
       "98            40.44                    2.178195   \n",
       "99            68.47                   72.331712   \n",
       "100           71.83                   78.697108   \n",
       "101           73.00                  141.509054   \n",
       "102           70.52                  101.093517   \n",
       "\n",
       "     Energy per Sentence (J/sentence)  Memory Usage (MB)  Model Size (MB)  \\\n",
       "0                           20.450349            7005.88      4992.689056   \n",
       "1                           18.718035            7005.88      4992.689056   \n",
       "2                           83.609322            7009.88      4992.689056   \n",
       "3                           83.609322            7009.88      4992.689056   \n",
       "4                           96.243709            7009.88      4992.689056   \n",
       "..                                ...                ...              ...   \n",
       "98                          17.425557            7011.88      4992.689056   \n",
       "99                          72.331712            7011.88      4992.689056   \n",
       "100                         78.697108            7011.88      4992.689056   \n",
       "101                        141.509054            7011.88      4992.689056   \n",
       "102                        101.093517            7011.88      4992.689056   \n",
       "\n",
       "     Overhead (MB)  \n",
       "0      2013.190944  \n",
       "1      2013.190944  \n",
       "2      2017.190944  \n",
       "3      2017.190944  \n",
       "4      2017.190944  \n",
       "..             ...  \n",
       "98     2019.190944  \n",
       "99     2019.190944  \n",
       "100    2019.190944  \n",
       "101    2019.190944  \n",
       "102    2019.190944  \n",
       "\n",
       "[103 rows x 27 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3357fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/fast_llm_inference\") \n",
    "\n",
    "from benchmark.tasks.summarization import SummarizationTask\n",
    "\n",
    "st = SummarizationTask()\n",
    "\n",
    "data = st.generate_prompts(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1352e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 / 512 prompts retained under 1200 tokens.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/fast_llm_inference\") \n",
    "\n",
    "from benchmark.tasks.summarization import SummarizationTask\n",
    "\n",
    "st = SummarizationTask()\n",
    "\n",
    "data = st.generate_prompts(512)\n",
    "\n",
    "# Prevent tokenizers or HF internals from using GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Disables GPU access for subprocesses\n",
    "\n",
    "# Load tokenizer (runs on CPU by default)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-32B-Instruct\", trust_remote_code=True)\n",
    "\n",
    "# Prepare prompt DataFrame\n",
    "df = pd.DataFrame(data[0], columns=[\"prompt\"])\n",
    "df[\"tokens\"] = df[\"prompt\"].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=False)))\n",
    "df[\"words\"] = df[\"prompt\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Filter prompts based on context window constraint\n",
    "max_input_tokens = 1200 \n",
    "df_filtered = df[df[\"tokens\"] <= max_input_tokens].reset_index(drop=True)\n",
    "\n",
    "print(f\"{len(df_filtered)} / {len(df)} prompts retained under {max_input_tokens} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3771081d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 / 256 prompts retained under 1970 tokens.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/fast_llm_inference\") \n",
    "\n",
    "from benchmark.tasks.summarization import SummarizationTask\n",
    "\n",
    "st = SummarizationTask()\n",
    "\n",
    "data = st.generate_prompts(256, max_words=1360)\n",
    "\n",
    "# Prevent tokenizers or HF internals from using GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Disables GPU access for subprocesses\n",
    "\n",
    "# Load tokenizer (runs on CPU by default)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\", trust_remote_code=True)\n",
    "\n",
    "# Prepare prompt DataFrame\n",
    "df = pd.DataFrame(data[0], columns=[\"prompt\"])\n",
    "df[\"tokens\"] = df[\"prompt\"].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=False)))\n",
    "df[\"words\"] = df[\"prompt\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Filter prompts based on context window constraint\n",
    "max_input_tokens = 1970\n",
    "df_filtered = df[df[\"tokens\"] <= max_input_tokens].reset_index(drop=True)\n",
    "\n",
    "print(f\"{len(df_filtered)} / {len(df)} prompts retained under {max_input_tokens} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18ce431d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1851"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1bf6785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3406135064833975"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].mean() / df['words'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5260104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17627.1552734375, 17.21401882171631)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, hf_hub_download, list_repo_files\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "def hf_model_size(repo_id: str, revision: str | None = None) -> Tuple[int, float, float]:\n",
    "    \"\"\"\n",
    "    Return (bytes, mebibytes, gibibytes) occupied by `repo_id` on Hugging Face.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    repo_id   : \"owner/model_name\" on HF Hub.\n",
    "    revision  : branch / tag / commit hash (default: main).\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> hf_model_size(\"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\")\n",
    "    (15321596485, 14609.49, 14.27)\n",
    "    \"\"\"\n",
    "    # 1) Try the fast path ─ total_size inside *.index.json\n",
    "    try:\n",
    "        index_path = hf_hub_download(\n",
    "            repo_id, \"model.safetensors.index.json\",\n",
    "            repo_type=\"model\", revision=revision, local_dir=\"/tmp\", local_dir_use_symlinks=False\n",
    "        )\n",
    "        with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            total = json.load(f).get(\"metadata\", {}).get(\"total_size\")\n",
    "            if isinstance(total, int) and total > 0:\n",
    "                mib = total / (1024**2)\n",
    "                gib = total / (1024**3)\n",
    "                return mib, gib\n",
    "    except Exception:\n",
    "        pass  # fall back to listing files\n",
    "\n",
    "    # 2) Fallback ─ sum all weight files\n",
    "    weight_exts = {\".bin\", \".safetensors\", \".pt\", \".pth\"}\n",
    "    size_sum = 0\n",
    "    for file_info in api.list_repo_files(repo_id, revision=revision, repo_type=\"model\"):\n",
    "        if Path(file_info).suffix in weight_exts:\n",
    "            # head() gives the object metadata without downloading\n",
    "            hf_obj = api.head(repo_id, file_info, revision=revision, repo_type=\"model\")\n",
    "            size_sum += hf_obj.size\n",
    "\n",
    "    mib = size_sum / (1024**2)\n",
    "    gib = size_sum / (1024**3)\n",
    "    return mib, gib\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_id = \"google/gemma-2-9b-it\"\n",
    "    print(hf_model_size(model_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77dd9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import sqlite3\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple, List, Dict\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sqlglot import parse_one\n",
    "import sqlparse\n",
    "\n",
    "LOOKUP_DIR = Path(\"/home/ubuntu/fast_llm_inference/benchmark/lookup\")\n",
    "\n",
    "class SQLTask:\n",
    "    \"\"\"SQL generation task for the Spider dataset with optional execution accuracy.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 tables_path: Union[str, Path, None] = None,\n",
    "                 db_root: Union[str, Path, None] = None,\n",
    "                 seed: int = 42):\n",
    "        random.seed(seed)\n",
    "        self.dataset = list(load_dataset(\"spider\", split=\"train\"))\n",
    "\n",
    "        self.tables_path: Path = Path(tables_path or LOOKUP_DIR / \"tables.json\")\n",
    "        if not self.tables_path.exists():\n",
    "            raise FileNotFoundError(f\"Spider tables.json not found at {self.tables_path}.\")\n",
    "\n",
    "        # Try to find or use given database path\n",
    "        if db_root is not None:\n",
    "            db_root_path = Path(db_root).expanduser().resolve()\n",
    "        else:\n",
    "            db_root_path = self.tables_path.parent / \"database\"\n",
    "\n",
    "        if db_root_path.is_dir():\n",
    "            self.db_root = db_root_path\n",
    "            self.execution_enabled = True\n",
    "        else:\n",
    "            self.db_root = None\n",
    "            self.execution_enabled = False\n",
    "            print(\"[SQLTask] Warning: database/ folder not found. Execution accuracy will be disabled.\")\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def _conn(self, db_id: str) -> sqlite3.Connection:\n",
    "        if not self.execution_enabled:\n",
    "            raise RuntimeError(\"Execution not available: no database folder found.\")\n",
    "        db_file = self.db_root / db_id / f\"{db_id}.sqlite\"\n",
    "        if not db_file.exists():\n",
    "            raise FileNotFoundError(f\"SQLite DB for db_id='{db_id}' not found at {db_file}.\")\n",
    "        return sqlite3.connect(f\"file:{db_file}?immutable=1\", uri=True, timeout=1.0, isolation_level=None)\n",
    "\n",
    "    def _exec(self, db_id: str, query: str) -> set:\n",
    "        if not self.execution_enabled:\n",
    "            raise RuntimeError(\"Execution disabled (no database folder).\")\n",
    "        cur = self._conn(db_id).cursor()\n",
    "        cur.execute(query.strip().rstrip(\";\"))\n",
    "        rows = cur.fetchall()\n",
    "        return {tuple(r) for r in rows}\n",
    "\n",
    "    def generate_prompts(self, num_examples: int = 100) -> Tuple[List[str], List[str], List[str]]:\n",
    "        sampled = random.sample(self.dataset, num_examples)\n",
    "        prompts    = [self._sql_prompt(ex) for ex in sampled]\n",
    "        references = [ex[\"query\"]         for ex in sampled]\n",
    "        db_ids     = [ex[\"db_id\"]         for ex in sampled]\n",
    "        return prompts, references, db_ids\n",
    "\n",
    "    def _table_columns(self, db_id: str) -> str:\n",
    "        with open(self.tables_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            tables_json = json.load(f)\n",
    "\n",
    "        for db in tables_json:\n",
    "            if db[\"db_id\"] == db_id:\n",
    "                table_names = db[\"table_names_original\"]\n",
    "                column_info = db[\"column_names_original\"]\n",
    "                table_columns = {t: [] for t in table_names}\n",
    "                for table_idx, col_name in column_info:\n",
    "                    if col_name == \"*\":\n",
    "                        continue\n",
    "                    table_columns[table_names[table_idx]].append(col_name)\n",
    "                return \"\\n\".join(\n",
    "                    f\"Table '{tbl}': columns = {', '.join(cols)}\" for tbl, cols in table_columns.items()\n",
    "                )\n",
    "\n",
    "        return f\"No schema found for db_id='{db_id}'.\"\n",
    "\n",
    "    def _sql_prompt(self, example: Dict) -> str:\n",
    "        question = example[\"question\"]\n",
    "        column_context = self._table_columns(example[\"db_id\"])\n",
    "\n",
    "        system_message = (\n",
    "            \"You are a SQL query generation assistant.  Given a natural language \"\n",
    "            \"question and the database schema, produce only the corresponding SQL \"\n",
    "            \"statement ending with a semicolon.  No explanations.\\n\\n\"\n",
    "        )\n",
    "\n",
    "        demo_block = (\n",
    "            \"### EXAMPLES\\n\"\n",
    "            \"Question:\\n\"\n",
    "            \"How many heads of the departments are older than 56?\\n\\n\"\n",
    "            \"Tables in the database:\\n\"\n",
    "            \"Table 'department': columns = Department_ID, Name, Creation, Ranking, Budget_in_Billions, Num_Employees\\n\"\n",
    "            \"Table 'head':       columns = head_ID, name, born_state, age\\n\"\n",
    "            \"Table 'management': columns = department_ID, head_ID, temporary_acting\\n\\n\"\n",
    "            \"SQL:\\n\"\n",
    "            \"SELECT COUNT(*) FROM head WHERE age > 56;\\n\\n\"\n",
    "        )\n",
    "\n",
    "        instruction = \"### INSTRUCTION\\nGenerate the SQL statement that answers the question.\\n\\n\"\n",
    "        input_block = (\n",
    "            \"### INPUT\\n\"\n",
    "            f\"Question:\\n{question}\\n\\n\"\n",
    "            \"Tables in the database:\\n\"\n",
    "            f\"{column_context}\\n\\n\"\n",
    "        )\n",
    "        output_header = \"### OUTPUT\\nSQL:\\n\"\n",
    "\n",
    "        return (\n",
    "            f\"### SYSTEM\\n{system_message}\\n\"\n",
    "            f\"{demo_block}\"\n",
    "            f\"{instruction}\"\n",
    "            f\"{input_block}\"\n",
    "            f\"{output_header}\"\n",
    "        )\n",
    "\n",
    "    def quality_metrics(self,\n",
    "                        generated: str,\n",
    "                        reference: str,\n",
    "                        db_id: str | None = None) -> dict[str, float]:\n",
    "\n",
    "        def ast_equal(sql1: str, sql2: str) -> int:\n",
    "            try:\n",
    "                return int(parse_one(sql1.lower()) == parse_one(sql2.lower()))\n",
    "            except Exception:\n",
    "                return 0\n",
    "\n",
    "        metrics = {\"AST_equal\": ast_equal(generated, reference)}\n",
    "\n",
    "        if db_id is not None and self.execution_enabled:\n",
    "            try:\n",
    "                # 1️⃣ Run full reference to get the “correct” result set\n",
    "                ref_res = self._exec(db_id, reference)\n",
    "\n",
    "                # 2️⃣ Split generated SQL into individual statements\n",
    "                stmts = [s.strip() for s in sqlparse.split(generated) if s.strip()]\n",
    "\n",
    "                # 3️⃣ Execute each one; if any matches ref_res, we’re good\n",
    "                exec_correct = 0.0\n",
    "                for stmt in stmts:\n",
    "                    try:\n",
    "                        gen_res = self._exec(db_id, stmt)\n",
    "                        if gen_res == ref_res:\n",
    "                            exec_correct = 1.0\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        # ignore bad statements\n",
    "                        continue\n",
    "\n",
    "                metrics[\"Exec_accuracy\"] = exec_correct\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] Execution error for db_id='{db_id}': {e}\")\n",
    "                metrics[\"Exec_accuracy\"] = 0.0\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e7b9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = SQLTask(db_root=\"/home/ubuntu/fast_llm_inference/benchmark/lookup/database\", tables_path=\"/home/ubuntu/fast_llm_inference/benchmark/lookup/tables.json\")\n",
    "prompts, refs, db_ids = task.generate_prompts(num_examples=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8d564d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Merging unique files from 'details'\n",
      "⚠️ Duplicate detected for 'vllm_gemma-2-9b-it_sql_batch_bs16' — kept: vllm_gemma-2-9b-it_sql_batch_bs16_97331b63.csv\n",
      "⚠️ Duplicate detected for 'vllm_Qwen2.5-7B-Instruct_sql_batch_bs16' — kept: vllm_Qwen2.5-7B-Instruct_sql_batch_bs16_31034d81.csv\n",
      "⚠️ Duplicate detected for 'vllm_gemma-2-9b-it_qa_batch_bs16' — kept: vllm_gemma-2-9b-it_qa_batch_bs16_34b1c6f1.csv\n",
      "⚠️ Duplicate detected for 'vllm_Mistral-7B-Instruct-v0.3_qa_batch_bs16' — kept: vllm_Mistral-7B-Instruct-v0.3_qa_batch_bs16_e95403f7.csv\n",
      "⚠️ Duplicate detected for 'vllm_Mistral-7B-Instruct-v0.3_summarization_batch_bs16' — kept: vllm_Mistral-7B-Instruct-v0.3_summarization_batch_bs16_b840bb9a.csv\n",
      "⚠️ Duplicate detected for 'vllm_Qwen2.5-7B-Instruct_summarization_batch_bs16' — kept: vllm_Qwen2.5-7B-Instruct_summarization_batch_bs16_b484770d.csv\n",
      "⚠️ Duplicate detected for 'vllm_Qwen2.5-7B-Instruct_qa_batch_bs16' — kept: vllm_Qwen2.5-7B-Instruct_qa_batch_bs16_473ad563.csv\n",
      "⚠️ Duplicate detected for 'vllm_Qwen2.5-3B-Instruct_summarization_batch_bs16' — kept: vllm_Qwen2.5-3B-Instruct_summarization_batch_bs16_ad50dd3a.csv\n",
      "⚠️ Duplicate detected for 'vllm_Mistral-7B-Instruct-v0.3_sql_batch_bs16' — kept: vllm_Mistral-7B-Instruct-v0.3_sql_batch_bs16_0c145654.csv\n",
      "\n",
      "📂 Merging unique files from 'readings'\n",
      "⚠️ Duplicate detected for 'vllm_Mistral-7B-Instruct-v0.3_qa_batch_bs16' — kept: ts_vllm_Mistral-7B-Instruct-v0.3_qa_batch_bs16_e95403f7.csv\n",
      "⚠️ Duplicate detected for 'vllm_gemma-2-9b-it_sql_batch_bs16' — kept: ts_vllm_gemma-2-9b-it_sql_batch_bs16_97331b63.csv\n",
      "⚠️ Duplicate detected for 'vllm_Mistral-7B-Instruct-v0.3_sql_batch_bs16' — kept: ts_vllm_Mistral-7B-Instruct-v0.3_sql_batch_bs16_0c145654.csv\n",
      "⚠️ Duplicate detected for 'vllm_Qwen2.5-7B-Instruct_summarization_batch_bs16' — kept: ts_vllm_Qwen2.5-7B-Instruct_summarization_batch_bs16_b484770d.csv\n",
      "⚠️ Duplicate detected for 'vllm_Qwen2.5-3B-Instruct_summarization_batch_bs16' — kept: ts_vllm_Qwen2.5-3B-Instruct_summarization_batch_bs16_ad50dd3a.csv\n",
      "⚠️ Duplicate detected for 'vllm_Mistral-7B-Instruct-v0.3_summarization_batch_bs16' — kept: ts_vllm_Mistral-7B-Instruct-v0.3_summarization_batch_bs16_b840bb9a.csv\n",
      "⚠️ Duplicate detected for 'vllm_Qwen2.5-7B-Instruct_sql_batch_bs16' — kept: ts_vllm_Qwen2.5-7B-Instruct_sql_batch_bs16_31034d81.csv\n",
      "⚠️ Duplicate detected for 'vllm_gemma-2-9b-it_qa_batch_bs16' — kept: ts_vllm_gemma-2-9b-it_qa_batch_bs16_34b1c6f1.csv\n",
      "⚠️ Duplicate detected for 'vllm_Qwen2.5-7B-Instruct_qa_batch_bs16' — kept: ts_vllm_Qwen2.5-7B-Instruct_qa_batch_bs16_473ad563.csv\n",
      "\n",
      "📂 Merging unique files from 'run_report'\n",
      "⚠️ Duplicate detected for 'vllm_gemma-2-9b-it_sql_batch_bs16' — kept: vllm_gemma-2-9b-it_sql_batch_bs16_97331b63.csv\n",
      "⚠️ Duplicate detected for 'vllm_Qwen2.5-7B-Instruct_sql_batch_bs16' — kept: vllm_Qwen2.5-7B-Instruct_sql_batch_bs16_31034d81.csv\n",
      "⚠️ Duplicate detected for 'vllm_gemma-2-9b-it_qa_batch_bs16' — kept: vllm_gemma-2-9b-it_qa_batch_bs16_34b1c6f1.csv\n",
      "⚠️ Duplicate detected for 'vllm_Mistral-7B-Instruct-v0.3_qa_batch_bs16' — kept: vllm_Mistral-7B-Instruct-v0.3_qa_batch_bs16_e95403f7.csv\n",
      "⚠️ Duplicate detected for 'vllm_Mistral-7B-Instruct-v0.3_summarization_batch_bs16' — kept: vllm_Mistral-7B-Instruct-v0.3_summarization_batch_bs16_b840bb9a.csv\n",
      "⚠️ Duplicate detected for 'vllm_Qwen2.5-7B-Instruct_summarization_batch_bs16' — kept: vllm_Qwen2.5-7B-Instruct_summarization_batch_bs16_b484770d.csv\n",
      "⚠️ Duplicate detected for 'vllm_Qwen2.5-7B-Instruct_qa_batch_bs16' — kept: vllm_Qwen2.5-7B-Instruct_qa_batch_bs16_473ad563.csv\n",
      "⚠️ Duplicate detected for 'vllm_Qwen2.5-3B-Instruct_summarization_batch_bs16' — kept: vllm_Qwen2.5-3B-Instruct_summarization_batch_bs16_ad50dd3a.csv\n",
      "⚠️ Duplicate detected for 'vllm_Mistral-7B-Instruct-v0.3_sql_batch_bs16' — kept: vllm_Mistral-7B-Instruct-v0.3_sql_batch_bs16_0c145654.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "# ─── 1️⃣ Setup ─────────────────────────────────────────────────────\n",
    "base_dir = \"/home/ubuntu/fast_llm_inference\"\n",
    "datasets = [\"RQ1\", \"RQ1_rerun\", \"RQ2_256\", \"RQ2_rerun\"]\n",
    "subdirs = [\"details\", \"readings\", \"run_report\"]\n",
    "output_dir = os.path.join(base_dir, \"RQ1_merged_256\")\n",
    "\n",
    "# Create clean target dir\n",
    "for subdir in subdirs:\n",
    "    os.makedirs(os.path.join(output_dir, subdir), exist_ok=True)\n",
    "\n",
    "# ─── 2️⃣ Normalize filenames (strip hash) ──────────────────────────\n",
    "def normalize_filename(path, is_reading=False):\n",
    "    fname = os.path.basename(path)\n",
    "    if is_reading:\n",
    "        fname = fname.replace(\"ts_\", \"\")\n",
    "    base = re.sub(r\"(_[a-f0-9]{8})?\\.csv$\", \"\", fname)\n",
    "    return base\n",
    "\n",
    "# ─── 3️⃣ Find all CSVs and group by normalized name ────────────────\n",
    "file_registry = {s: defaultdict(list) for s in subdirs}\n",
    "\n",
    "for dataset in datasets:\n",
    "    for subdir in subdirs:\n",
    "        dir_path = os.path.join(base_dir, dataset, subdir)\n",
    "        is_reading = (subdir == \"readings\")\n",
    "        for path in glob(f\"{dir_path}/*.csv\"):\n",
    "            key = normalize_filename(path, is_reading)\n",
    "            file_registry[subdir][key].append(path)\n",
    "\n",
    "# ─── 4️⃣ Copy first file per key to merged folder ──────────────────\n",
    "for subdir in subdirs:\n",
    "    merged_subdir = os.path.join(output_dir, subdir)\n",
    "    print(f\"\\n📂 Merging unique files from '{subdir}'\")\n",
    "    for key, files in file_registry[subdir].items():\n",
    "        src = files[0]  # take the first file regardless of duplicates\n",
    "        dst = os.path.join(merged_subdir, os.path.basename(src))\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "        if len(files) > 1:\n",
    "            print(f\"⚠️ Duplicate detected for '{key}' — kept: {os.path.basename(src)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1bae5cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Missing (incomplete) runs ===\n",
      "Empty DataFrame\n",
      "Columns: [Model, qa, sql, summarization, Complete]\n",
      "Index: []\n",
      "\n",
      "=== Duplicate CSVs (hash-agnostic) ===\n",
      "No duplicates 🎉\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "DETAILS_DIR = Path(\"/home/ubuntu/fast_llm_inference/RQ1_merged_256/details\")\n",
    "\n",
    "CONFIG_MODELS = \"\"\"\n",
    "# Qwen2.5\n",
    "- Qwen/Qwen2.5-1.5B-Instruct\n",
    "- Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4\n",
    "- Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int8\n",
    "- Qwen/Qwen2.5-3B-Instruct\n",
    "- Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\n",
    "- Qwen/Qwen2.5-3B-Instruct-GPTQ-Int8\n",
    "- Qwen/Qwen2.5-7B-Instruct\n",
    "- Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\n",
    "- Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\n",
    "- Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\n",
    "- Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8\n",
    "- Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\n",
    "\n",
    "# Mistral\n",
    "- mistralai/Mistral-7B-Instruct-v0.3\n",
    "- marinarosell/Mistral-7B-Instruct-v0.3-GPTQ-8bit-gs128\n",
    "- marinarosell/Mistral-7B-Instruct-v0.3-GPTQ-4bit-gs128\n",
    "- dwetzel/Mistral-Small-24B-Instruct-2501-GPTQ-INT4\n",
    "\n",
    "# Gemma-2\n",
    "- google/gemma-2-2b-it\n",
    "- marcinbrzezanski/gemma-2-2b-it-gptq\n",
    "- RedHatAI/gemma-2-2b-it-quantized.w8a16\n",
    "- google/gemma-2-9b-it\n",
    "- shuyuej/gemma-2-9b-it-GPTQ\n",
    "- RedHatAI/gemma-2-9b-it-quantized.w8a16\n",
    "- shuyuej/gemma-2-27b-it-GPTQ\n",
    "\n",
    "# LLaMA 3.x\n",
    "- meta-llama/Llama-3.2-1B-Instruct\n",
    "- clowman/Llama-3.2-1B-Instruct-GPTQ-Int4\n",
    "- clowman/Llama-3.2-1B-Instruct-GPTQ-Int8\n",
    "- meta-llama/Llama-3.2-3B-Instruct\n",
    "- clowman/Llama-3.2-3B-Instruct-GPTQ-Int8\n",
    "- clowman/Llama-3.2-3B-Instruct-GPTQ-Int4\n",
    "- meta-llama/Llama-3.1-8B-Instruct\n",
    "- clowman/Llama-3.1-8B-Instruct-GPTQ-Int4\n",
    "- clowman/Llama-3.1-8B-Instruct-GPTQ-Int8\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "expected_models = [m.split(\"/\")[-1] for m in yaml.safe_load(CONFIG_MODELS)]\n",
    "tasks = [\"qa\", \"sql\", \"summarization\"]\n",
    "\n",
    "# 1️⃣  Scan all CSVs\n",
    "existing_files = list(DETAILS_DIR.glob(\"*.csv\"))\n",
    "\n",
    "# helper → strip “vllm_” prefix and trailing “_<8-hex>.csv”\n",
    "def normalize(fname: str) -> str:\n",
    "    core = re.sub(r\"^vllm_\", \"\", fname)\n",
    "    return re.sub(r\"_[a-f0-9]{8}\\.csv$\", \"\", core)\n",
    "\n",
    "# Build presence map and duplicate map\n",
    "present = {m: {t: False for t in tasks} for m in expected_models}\n",
    "dupes: dict[str, list[str]] = {}\n",
    "\n",
    "for f in existing_files:\n",
    "    norm = normalize(f.name)                       # e.g. Llama-..._qa_batch_bs16\n",
    "    dupes.setdefault(norm, []).append(f.name)      # collect for duplicate check\n",
    "\n",
    "    parts = norm.split(\"_\")\n",
    "    if len(parts) < 4:\n",
    "        continue\n",
    "    model_id = \"_\".join(parts[:-3])\n",
    "    task      = parts[-3]\n",
    "    if model_id in present and task in present[model_id]:\n",
    "        present[model_id][task] = True\n",
    "\n",
    "# 2️⃣  “Which runs are still missing?”\n",
    "records = []\n",
    "for model in expected_models:\n",
    "    st = present[model]\n",
    "    records.append(\n",
    "        {\"Model\": model, \"qa\": st[\"qa\"], \"sql\": st[\"sql\"],\n",
    "         \"summarization\": st[\"summarization\"], \"Complete\": all(st.values())}\n",
    "    )\n",
    "missing_df = pd.DataFrame(records).sort_values(\"Model\")\n",
    "\n",
    "# 3️⃣  “Which runs are duplicated (same model+task, different hashes)?”\n",
    "dup_records = [\n",
    "    {\"Normalized name\": k, \"Count\": len(v), \"Files\": \" | \".join(v)}\n",
    "    for k, v in dupes.items() if len(v) > 1\n",
    "]\n",
    "\n",
    "if dup_records:\n",
    "    dupes_df = pd.DataFrame(dup_records).sort_values(\"Count\", ascending=False)\n",
    "else:\n",
    "    dupes_df = pd.DataFrame(columns=[\"Normalized name\", \"Count\", \"Files\"])\n",
    "\n",
    "\n",
    "# 4️⃣  Show / save results\n",
    "print(\"\\n=== Missing (incomplete) runs ===\")\n",
    "print(missing_df[~missing_df[\"Complete\"]])\n",
    "\n",
    "print(\"\\n=== Duplicate CSVs (hash-agnostic) ===\")\n",
    "print(dupes_df if not dupes_df.empty else \"No duplicates 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74018817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀  Merged 96 files → /home/ubuntu/fast_llm_inference/RQ1_merged_256/merged_details.csv  (24576 rows)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# merge_benchmark_details.py\n",
    "# Usage:  python merge_benchmark_details.py /home/ubuntu/fast_llm_inference/RQ1_merged/details\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ─────────────────────────────────────── config\n",
    "DETAILS_DIR     = Path(\"/home/ubuntu/fast_llm_inference/RQ1_merged_256/details\")\n",
    "OUT_PATH        = DETAILS_DIR.parent / \"merged_details.csv\"\n",
    "\n",
    "# All columns we want in the final file (order matters)\n",
    "FINAL_COLS = [\n",
    "    \"backend\", \"model\", \"task\",\n",
    "    \"generated_answer\", \"reference_answer\",\n",
    "    \"generation_time\", \"tokens_generated\", \"sentences_generated\",\n",
    "    \"ATL\", \"GL\", \"TPS\", \"SPS\",\n",
    "    \"energy_per_token\", \"energy_per_sentence\",\n",
    "    \"exact_match\", \"F1_score\",\n",
    "    \"AST_equal\", \"Normalized_equal\",\n",
    "    \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"\n",
    "]\n",
    "\n",
    "# Regex to pull <backend>, <model>, <task> from the filename\n",
    "PAT = re.compile(\n",
    "    r\"\"\"^(?P<backend>[^_]+)_           # vllm, tgi …\n",
    "        (?P<model>.+?)_                # greedy until _<task>_\n",
    "        (?P<task>qa|sql|summarization) # capture task\n",
    "        _[^/]*\\.csv$                   # rest is scenario, hash …\n",
    "    \"\"\", re.X | re.I)\n",
    "\n",
    "def load_one(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read a single details-CSV and normalise its columns.\"\"\"\n",
    "    m = PAT.match(path.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse '{path.name}'\")\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Drop prompt – we don't need it in the merged file\n",
    "    df = df.drop(columns=[c for c in df.columns if c.lower() == \"prompt\"], errors=\"ignore\")\n",
    "\n",
    "    # Inject backend / model / task from filename\n",
    "    for k, v in m.groupdict().items():\n",
    "        df[k] = v\n",
    "\n",
    "    # Make sure every expected column exists; missing ⇒ NaN\n",
    "    for col in FINAL_COLS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # Keep only the final schema & order\n",
    "    return df[FINAL_COLS]\n",
    "\n",
    "# ─────────────────────────────────────── main\n",
    "all_csvs = sorted(DETAILS_DIR.glob(\"*.csv\"))\n",
    "if not all_csvs:\n",
    "    sys.exit(f\"No CSVs found in {DETAILS_DIR}\")\n",
    "\n",
    "merged = pd.concat([load_one(p) for p in all_csvs], ignore_index=True)\n",
    "merged.to_csv(OUT_PATH, index=False)\n",
    "print(f\"🚀  Merged {len(all_csvs)} files → {OUT_PATH}  ({len(merged)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c76d14f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_task = SQLTask(\n",
    "    db_root=\"/home/ubuntu/fast_llm_inference/benchmark/lookup/database\",\n",
    "    tables_path=\"/home/ubuntu/fast_llm_inference/benchmark/lookup/tables.json\"\n",
    ")\n",
    "df_sql_id = pd.DataFrame(sql_task.generate_prompts(num_examples=256)[1:])\n",
    "df_sql_id = df_sql_id.T            # flips rows↔columns\n",
    "df_sql_id.columns = [\"reference\", \"db_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "963e3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/home/ubuntu/fast_llm_inference/RQ1_merged_256/merged_details.csv\"\n",
    "merged = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9da932e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3838/1186527573.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['music_2' 'insurance_fnol' 'bike_1' ... 'music_1' 'cre_Doc_Tracking_DB'\n",
      " 'baseball_1']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  merged.loc[sql_mask, \"db_id\"] = merged.loc[sql_mask, \"reference_answer\"].map(ref2db)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1️⃣  Build a mapping:   reference → db_id\n",
    "ref2db = dict(zip(df_sql_id[\"reference\"], df_sql_id[\"db_id\"]))\n",
    "\n",
    "# 2️⃣  Initialise the column (optional; the map will create NaNs automatically)\n",
    "merged[\"db_id\"] = np.nan\n",
    "\n",
    "# 3️⃣  Fill db_id only for the SQL rows\n",
    "sql_mask = merged[\"task\"].str.lower() == \"sql\"\n",
    "merged.loc[sql_mask, \"db_id\"] = merged.loc[sql_mask, \"reference_answer\"].map(ref2db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5368e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Mask SQL rows\n",
    "sql_mask = merged[\"task\"].str.lower() == \"sql\"\n",
    "\n",
    "# 2️⃣ Run quality_metrics on each SQL row, expand dict → columns\n",
    "metrics_df = merged.loc[sql_mask].apply(\n",
    "    lambda row: pd.Series(\n",
    "        sql_task.quality_metrics(\n",
    "            generated=row[\"generated_answer\"],\n",
    "            reference=row[\"reference_answer\"],\n",
    "            db_id=row[\"db_id\"]\n",
    "        )\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 3️⃣ Assign those metric-columns back into merged\n",
    "merged.loc[sql_mask, metrics_df.columns] = metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99177832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5234375"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[(merged['model'] == \"Mistral-Small-24B-Instruct-2501-GPTQ-INT4\") & (merged['task'] == \"sql\")][\"Exec_accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ccfbc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6015625"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[(merged['model'] == \"gemma-2-9b-it\") & (merged['task'] == \"sql\")][\"Exec_accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee05a062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6484375"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[(merged['model'] == \"gemma-2-27b-it-GPTQ\") & (merged['task'] == \"sql\")][\"Exec_accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f28c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Saved: /home/ubuntu/fast_llm_inference/RQ1_merged_256/merged_run_report_info_with_gpu_ci95.csv\n",
      "Matched:   96\n",
      "Unmatched: 0\n",
      "Skipped:   0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ─── 1️⃣ Load df_report ──────────────────────────────────────────\n",
    "df_report = pd.read_csv(\"/home/ubuntu/fast_llm_inference/RQ1_merged_256/merged_run_report_info.csv\")\n",
    "df_report[\"gpu_util_pct\"] = None\n",
    "df_report[\"gpu_util_pct_ci95\"] = None\n",
    "\n",
    "# ─── 2️⃣ File listing ────────────────────────────────────────────\n",
    "readings_dir = \"/home/ubuntu/fast_llm_inference/RQ1_merged_256/readings\"\n",
    "files = [f for f in os.listdir(readings_dir) if f.endswith(\".csv\") and f.startswith(\"ts_\")]\n",
    "\n",
    "matched, unmatched, skipped = 0, 0, 0\n",
    "\n",
    "# ─── 3️⃣ Process each reading file ───────────────────────────────\n",
    "for fname in files:\n",
    "    path = os.path.join(readings_dir, fname)\n",
    "    stem = fname.removesuffix(\".csv\")\n",
    "\n",
    "    # Remove ts_ prefix\n",
    "    if not stem.startswith(\"ts_\"):\n",
    "        skipped += 1\n",
    "        continue\n",
    "    stem = stem[3:]\n",
    "\n",
    "    parts = stem.split(\"_\")\n",
    "\n",
    "    if len(parts) < 5:\n",
    "        print(f\"❌ Too few parts in filename: {fname}\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    # Extract metadata\n",
    "    backend  = parts[0].strip().lower()\n",
    "    model = parts[1].strip()\n",
    "    task     = parts[2].strip().lower()\n",
    "    scenario = parts[3].strip().lower()\n",
    "\n",
    "    # Validate\n",
    "    valid_tasks = {\"sql\", \"qa\", \"summarization\"}\n",
    "    valid_scenarios = {\"single\", \"batch\", \"server\"}\n",
    "    valid_backends = {\"vllm\", \"tgi\", \"sglang\", \"lmdeploy\"}\n",
    "\n",
    "    if task not in valid_tasks or scenario not in valid_scenarios or backend not in valid_backends:\n",
    "        print(f\"⚠️ Unknown task or scenario in {fname}\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    # Read readings file\n",
    "    try:\n",
    "        df_reading = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not read {fname}: {e}\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    if \"gpu_util_pct\" not in df_reading.columns:\n",
    "        print(f\"⚠️ Skipping {fname} — 'gpu_util_pct' missing\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    # Compute mean and CI95\n",
    "    gpu_vals = df_reading[\"gpu_util_pct\"].dropna()\n",
    "    mean_val = gpu_vals.mean()\n",
    "    std_val  = gpu_vals.std()\n",
    "    n_val    = gpu_vals.count()\n",
    "    ci95_val = 1.96 * std_val / (n_val ** 0.5) if n_val > 1 else 0.0\n",
    "\n",
    "    # Find corresponding row\n",
    "    mask = (\n",
    "        (df_report[\"model_name\"] == model)\n",
    "        & (df_report[\"task\"].str.lower() == task)\n",
    "        & (df_report[\"scenario\"].str.lower() == scenario)\n",
    "        & (df_report[\"backend\"].str.lower() == backend)\n",
    "    )\n",
    "\n",
    "    if mask.sum() == 1:\n",
    "        df_report.loc[mask, \"gpu_util_pct\"] = mean_val\n",
    "        df_report.loc[mask, \"gpu_util_pct_ci95\"] = ci95_val\n",
    "        matched += 1\n",
    "    elif mask.sum() == 0:\n",
    "        print(f\"❌ No match for model={model}, task={task}, scenario={scenario}, backend={backend}\")\n",
    "        unmatched += 1\n",
    "    else:\n",
    "        print(f\"⚠️ Multiple matches for model={model}, task={task}, scenario={scenario}, backend={backend}\")\n",
    "        unmatched += 1\n",
    "\n",
    "# ─── 4️⃣ Save updated report ─────────────────────────────────────\n",
    "out_path = \"/home/ubuntu/fast_llm_inference/RQ1_merged_256/merged_run_report_info_with_gpu_ci95.csv\"\n",
    "df_report.to_csv(out_path, index=False)\n",
    "\n",
    "# ─── 5️⃣ Summary ─────────────────────────────────────────────────\n",
    "print(f\"✅ Done. Saved: {out_path}\")\n",
    "print(f\"Matched:   {matched}\")\n",
    "print(f\"Unmatched: {unmatched}\")\n",
    "print(f\"Skipped:   {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c826e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_size_mb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>13824.507812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>13824.507812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>13824.507812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Mistral-Small-24B-Instruct-2501-GPTQ-INT4</td>\n",
       "      <td>13492.045288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Mistral-Small-24B-Instruct-2501-GPTQ-INT4</td>\n",
       "      <td>13492.045288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Mistral-Small-24B-Instruct-2501-GPTQ-INT4</td>\n",
       "      <td>13492.045288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Qwen2.5-14B-Instruct-GPTQ-Int8</td>\n",
       "      <td>15875.072266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Qwen2.5-14B-Instruct-GPTQ-Int8</td>\n",
       "      <td>15875.072266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Qwen2.5-14B-Instruct-GPTQ-Int8</td>\n",
       "      <td>15875.072266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Qwen2.5-32B-Instruct-GPTQ-Int4</td>\n",
       "      <td>18447.634766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Qwen2.5-32B-Instruct-GPTQ-Int4</td>\n",
       "      <td>18447.634766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Qwen2.5-32B-Instruct-GPTQ-Int4</td>\n",
       "      <td>18447.634766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Qwen2.5-7B-Instruct</td>\n",
       "      <td>14525.635742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Qwen2.5-7B-Instruct</td>\n",
       "      <td>14525.635742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Qwen2.5-7B-Instruct</td>\n",
       "      <td>14525.635742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>gemma-2-27b-it-GPTQ</td>\n",
       "      <td>17418.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>gemma-2-27b-it-GPTQ</td>\n",
       "      <td>17418.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>gemma-2-27b-it-GPTQ</td>\n",
       "      <td>17418.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>gemma-2-9b-it</td>\n",
       "      <td>17627.155273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>gemma-2-9b-it</td>\n",
       "      <td>17627.155273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>gemma-2-9b-it</td>\n",
       "      <td>17627.155273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   model_name  model_size_mb\n",
       "33                   Mistral-7B-Instruct-v0.3   13824.507812\n",
       "34                   Mistral-7B-Instruct-v0.3   13824.507812\n",
       "35                   Mistral-7B-Instruct-v0.3   13824.507812\n",
       "36  Mistral-Small-24B-Instruct-2501-GPTQ-INT4   13492.045288\n",
       "37  Mistral-Small-24B-Instruct-2501-GPTQ-INT4   13492.045288\n",
       "38  Mistral-Small-24B-Instruct-2501-GPTQ-INT4   13492.045288\n",
       "51             Qwen2.5-14B-Instruct-GPTQ-Int8   15875.072266\n",
       "52             Qwen2.5-14B-Instruct-GPTQ-Int8   15875.072266\n",
       "53             Qwen2.5-14B-Instruct-GPTQ-Int8   15875.072266\n",
       "54             Qwen2.5-32B-Instruct-GPTQ-Int4   18447.634766\n",
       "55             Qwen2.5-32B-Instruct-GPTQ-Int4   18447.634766\n",
       "56             Qwen2.5-32B-Instruct-GPTQ-Int4   18447.634766\n",
       "72                        Qwen2.5-7B-Instruct   14525.635742\n",
       "73                        Qwen2.5-7B-Instruct   14525.635742\n",
       "74                        Qwen2.5-7B-Instruct   14525.635742\n",
       "75                        gemma-2-27b-it-GPTQ   17418.012695\n",
       "76                        gemma-2-27b-it-GPTQ   17418.012695\n",
       "77                        gemma-2-27b-it-GPTQ   17418.012695\n",
       "93                              gemma-2-9b-it   17627.155273\n",
       "94                              gemma-2-9b-it   17627.155273\n",
       "95                              gemma-2-9b-it   17627.155273"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_include = [\n",
    "    \"Mistral-7B-Instruct-v0.3\", \"Mistral-Small-24B-Instruct-2501-GPTQ-INT4\",\n",
    "    \"gemma-2-9b-it\", \"gemma-2-27b-it-GPTQ\",\n",
    "    \"Qwen2.5-7B-Instruct\", \"Qwen2.5-14B-Instruct-GPTQ-Int8\", \"Qwen2.5-32B-Instruct-GPTQ-Int4\"\n",
    "]\n",
    "\n",
    "filtered_sizes = df_report[df_report['model_name'].isin(model_include)][['model_name', 'model_size_mb']]\n",
    "\n",
    "filtered_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e25bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per prompt:\n",
      "  • Summarization: 1166.49\n",
      "  • SQL:           326.90\n",
      "  • QA:            311.30\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ─── Adjust path for imports ───\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "\n",
    "from benchmark.tasks.summarization import SummarizationTask\n",
    "from benchmark.tasks.sql import SQLTask\n",
    "from benchmark.tasks.qa import QATask\n",
    "\n",
    "# ─── Load tokenizer ───\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", use_fast=True)\n",
    "\n",
    "# ─── Define a helper to compute avg token length ───\n",
    "def average_tokens(prompts, tokenizer):\n",
    "    return sum(len(tokenizer(p, add_special_tokens=False)[\"input_ids\"]) for p in prompts) / len(prompts)\n",
    "\n",
    "# ─── Summarization ───\n",
    "st = SummarizationTask()\n",
    "sum_prompts = st.generate_prompts(256)[0]\n",
    "avg_sum = average_tokens(sum_prompts, tokenizer)\n",
    "\n",
    "# ─── SQL ───\n",
    "sqlt = SQLTask()\n",
    "sql_prompts = sqlt.generate_prompts(256)[0]\n",
    "avg_sql = average_tokens(sql_prompts, tokenizer)\n",
    "\n",
    "# ─── QA ───\n",
    "qat = QATask()\n",
    "qa_prompts = qat.generate_prompts(256)[0]\n",
    "avg_qa = average_tokens(qa_prompts, tokenizer)\n",
    "\n",
    "# ─── Print results ───\n",
    "print(f\"Average tokens per prompt:\")\n",
    "print(f\"  • Summarization: {avg_sum:.2f}\")\n",
    "print(f\"  • SQL:           {avg_sql:.2f}\")\n",
    "print(f\"  • QA:            {avg_qa:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "147276e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt', 'generated_answer', 'reference_answer', 'generation_time',\n",
       "       'tokens_generated', 'sentences_generated', 'ATL', 'GL', 'TPS', 'SPS',\n",
       "       'energy_per_token', 'energy_per_sentence', 'ROUGE-1', 'ROUGE-2',\n",
       "       'ROUGE-L', 'engine', 'model', 'task', 'use_case', 'AST_equal',\n",
       "       'exact_match', 'F1_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the folder with CSVs\n",
    "data_dir = Path(\"/home/ubuntu/fast_llm_inference/RQ3/details\")\n",
    "all_files = list(data_dir.glob(\"*.csv\"))\n",
    "\n",
    "# Filter for *_single_*.csv files\n",
    "single_files = [f for f in all_files if \"_single_\" in f.stem]\n",
    "\n",
    "# Load and annotate each CSV\n",
    "df_list = []\n",
    "for f in single_files:\n",
    "    parts = f.stem.split(\"_\")\n",
    "    if len(parts) < 4:\n",
    "        continue\n",
    "    engine = parts[0]\n",
    "    model = parts[1]\n",
    "    task = parts[2]\n",
    "    use_case = parts[3]\n",
    "\n",
    "    df = pd.read_csv(f)\n",
    "    df[\"engine\"] = engine\n",
    "    df[\"model\"] = model\n",
    "    df[\"task\"] = task\n",
    "    df[\"use_case\"] = use_case\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate into one DataFrame\n",
    "merged_single_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "merged_single_df.to_csv(\"/home/ubuntu/fast_llm_inference/RQ3/merged_single_details.csv\", index=False)\n",
    "\n",
    "merged_single_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf742e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>generation_time</th>\n",
       "      <th>tokens_generated</th>\n",
       "      <th>sentences_generated</th>\n",
       "      <th>ATL</th>\n",
       "      <th>GL</th>\n",
       "      <th>TPS</th>\n",
       "      <th>SPS</th>\n",
       "      <th>...</th>\n",
       "      <th>engine</th>\n",
       "      <th>model</th>\n",
       "      <th>task</th>\n",
       "      <th>use_case</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT Band.Firstname, Band.Lastname FROM Perf...</td>\n",
       "      <td>SELECT T2.firstname ,  T2.lastname FROM Perfor...</td>\n",
       "      <td>0.185022</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.014232</td>\n",
       "      <td>0.185022</td>\n",
       "      <td>70.26</td>\n",
       "      <td>10.81</td>\n",
       "      <td>...</td>\n",
       "      <td>vllm</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>sql</td>\n",
       "      <td>batch</td>\n",
       "      <td>bs32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT policy_type_code FROM Available_Policies</td>\n",
       "      <td>SELECT DISTINCT t3.policy_type_code FROM custo...</td>\n",
       "      <td>0.185022</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015419</td>\n",
       "      <td>0.185022</td>\n",
       "      <td>64.86</td>\n",
       "      <td>5.40</td>\n",
       "      <td>...</td>\n",
       "      <td>vllm</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>sql</td>\n",
       "      <td>batch</td>\n",
       "      <td>bs32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT station.id, station.name</td>\n",
       "      <td>SELECT DISTINCT T1.id ,  T1.name FROM station ...</td>\n",
       "      <td>0.185022</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.020558</td>\n",
       "      <td>0.185022</td>\n",
       "      <td>48.64</td>\n",
       "      <td>10.81</td>\n",
       "      <td>...</td>\n",
       "      <td>vllm</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>sql</td>\n",
       "      <td>batch</td>\n",
       "      <td>bs32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT customer_name FROM Customers WHERE cust...</td>\n",
       "      <td>SELECT customer_name FROM customers WHERE cust...</td>\n",
       "      <td>0.185022</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.185022</td>\n",
       "      <td>97.29</td>\n",
       "      <td>5.40</td>\n",
       "      <td>...</td>\n",
       "      <td>vllm</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>sql</td>\n",
       "      <td>batch</td>\n",
       "      <td>bs32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT technician.Name</td>\n",
       "      <td>SELECT T3.Name FROM repair_assignment AS T1 JO...</td>\n",
       "      <td>0.185022</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030837</td>\n",
       "      <td>0.185022</td>\n",
       "      <td>32.43</td>\n",
       "      <td>5.40</td>\n",
       "      <td>...</td>\n",
       "      <td>vllm</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>sql</td>\n",
       "      <td>batch</td>\n",
       "      <td>bs32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35835</th>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT Advisor FROM Student GROUP BY Advisor H...</td>\n",
       "      <td>SELECT Advisor FROM STUDENT GROUP BY Advisor H...</td>\n",
       "      <td>0.138727</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006032</td>\n",
       "      <td>0.138727</td>\n",
       "      <td>165.79</td>\n",
       "      <td>7.21</td>\n",
       "      <td>...</td>\n",
       "      <td>vllm</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>sql</td>\n",
       "      <td>batch</td>\n",
       "      <td>bs64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35836</th>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT state, SUM(acc_bal) as total_balance</td>\n",
       "      <td>SELECT sum(acc_bal) ,  state FROM customer WHE...</td>\n",
       "      <td>0.138727</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.138727</td>\n",
       "      <td>108.13</td>\n",
       "      <td>7.21</td>\n",
       "      <td>...</td>\n",
       "      <td>vllm</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>sql</td>\n",
       "      <td>batch</td>\n",
       "      <td>bs64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35837</th>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT artist_name, most_popular_in</td>\n",
       "      <td>SELECT artist_name FROM song WHERE resolution ...</td>\n",
       "      <td>0.138727</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011561</td>\n",
       "      <td>0.138727</td>\n",
       "      <td>86.50</td>\n",
       "      <td>7.21</td>\n",
       "      <td>...</td>\n",
       "      <td>vllm</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>sql</td>\n",
       "      <td>batch</td>\n",
       "      <td>bs64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35838</th>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT Employees.Employee_ID, COUNT(All_Docume...</td>\n",
       "      <td>SELECT Destroyed_by_Employee_ID ,  count(*) FR...</td>\n",
       "      <td>0.138727</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.138727</td>\n",
       "      <td>237.88</td>\n",
       "      <td>14.42</td>\n",
       "      <td>...</td>\n",
       "      <td>vllm</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>sql</td>\n",
       "      <td>batch</td>\n",
       "      <td>bs64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35839</th>\n",
       "      <td>### SYSTEM\\nYou are a SQL query generation ass...</td>\n",
       "      <td>SELECT player.name_first, player.name_last</td>\n",
       "      <td>SELECT T2.name_first ,  T2.name_last FROM sala...</td>\n",
       "      <td>0.138727</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.138727</td>\n",
       "      <td>93.71</td>\n",
       "      <td>14.42</td>\n",
       "      <td>...</td>\n",
       "      <td>vllm</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>sql</td>\n",
       "      <td>batch</td>\n",
       "      <td>bs64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35840 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  prompt  \\\n",
       "0      ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "1      ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "2      ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "3      ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "4      ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "...                                                  ...   \n",
       "35835  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "35836  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "35837  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "35838  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "35839  ### SYSTEM\\nYou are a SQL query generation ass...   \n",
       "\n",
       "                                        generated_answer  \\\n",
       "0      SELECT Band.Firstname, Band.Lastname FROM Perf...   \n",
       "1        SELECT policy_type_code FROM Available_Policies   \n",
       "2                        SELECT station.id, station.name   \n",
       "3      SELECT customer_name FROM Customers WHERE cust...   \n",
       "4                                 SELECT technician.Name   \n",
       "...                                                  ...   \n",
       "35835  SELECT Advisor FROM Student GROUP BY Advisor H...   \n",
       "35836        SELECT state, SUM(acc_bal) as total_balance   \n",
       "35837                SELECT artist_name, most_popular_in   \n",
       "35838  SELECT Employees.Employee_ID, COUNT(All_Docume...   \n",
       "35839         SELECT player.name_first, player.name_last   \n",
       "\n",
       "                                        reference_answer  generation_time  \\\n",
       "0      SELECT T2.firstname ,  T2.lastname FROM Perfor...         0.185022   \n",
       "1      SELECT DISTINCT t3.policy_type_code FROM custo...         0.185022   \n",
       "2      SELECT DISTINCT T1.id ,  T1.name FROM station ...         0.185022   \n",
       "3      SELECT customer_name FROM customers WHERE cust...         0.185022   \n",
       "4      SELECT T3.Name FROM repair_assignment AS T1 JO...         0.185022   \n",
       "...                                                  ...              ...   \n",
       "35835  SELECT Advisor FROM STUDENT GROUP BY Advisor H...         0.138727   \n",
       "35836  SELECT sum(acc_bal) ,  state FROM customer WHE...         0.138727   \n",
       "35837  SELECT artist_name FROM song WHERE resolution ...         0.138727   \n",
       "35838  SELECT Destroyed_by_Employee_ID ,  count(*) FR...         0.138727   \n",
       "35839  SELECT T2.name_first ,  T2.name_last FROM sala...         0.138727   \n",
       "\n",
       "       tokens_generated  sentences_generated       ATL        GL     TPS  \\\n",
       "0                    13                    2  0.014232  0.185022   70.26   \n",
       "1                    12                    1  0.015419  0.185022   64.86   \n",
       "2                     9                    2  0.020558  0.185022   48.64   \n",
       "3                    18                    1  0.010279  0.185022   97.29   \n",
       "4                     6                    1  0.030837  0.185022   32.43   \n",
       "...                 ...                  ...       ...       ...     ...   \n",
       "35835                23                    1  0.006032  0.138727  165.79   \n",
       "35836                15                    1  0.009248  0.138727  108.13   \n",
       "35837                12                    1  0.011561  0.138727   86.50   \n",
       "35838                33                    2  0.004204  0.138727  237.88   \n",
       "35839                13                    2  0.010671  0.138727   93.71   \n",
       "\n",
       "         SPS  ...  engine                     model  task use_case batch_size  \\\n",
       "0      10.81  ...    vllm  Mistral-7B-Instruct-v0.3   sql    batch       bs32   \n",
       "1       5.40  ...    vllm  Mistral-7B-Instruct-v0.3   sql    batch       bs32   \n",
       "2      10.81  ...    vllm  Mistral-7B-Instruct-v0.3   sql    batch       bs32   \n",
       "3       5.40  ...    vllm  Mistral-7B-Instruct-v0.3   sql    batch       bs32   \n",
       "4       5.40  ...    vllm  Mistral-7B-Instruct-v0.3   sql    batch       bs32   \n",
       "...      ...  ...     ...                       ...   ...      ...        ...   \n",
       "35835   7.21  ...    vllm  Mistral-7B-Instruct-v0.3   sql    batch       bs64   \n",
       "35836   7.21  ...    vllm  Mistral-7B-Instruct-v0.3   sql    batch       bs64   \n",
       "35837   7.21  ...    vllm  Mistral-7B-Instruct-v0.3   sql    batch       bs64   \n",
       "35838  14.42  ...    vllm  Mistral-7B-Instruct-v0.3   sql    batch       bs64   \n",
       "35839  14.42  ...    vllm  Mistral-7B-Instruct-v0.3   sql    batch       bs64   \n",
       "\n",
       "      ROUGE-1 ROUGE-2 ROUGE-L  exact_match  F1_score  \n",
       "0         NaN     NaN     NaN          NaN       NaN  \n",
       "1         NaN     NaN     NaN          NaN       NaN  \n",
       "2         NaN     NaN     NaN          NaN       NaN  \n",
       "3         NaN     NaN     NaN          NaN       NaN  \n",
       "4         NaN     NaN     NaN          NaN       NaN  \n",
       "...       ...     ...     ...          ...       ...  \n",
       "35835     NaN     NaN     NaN          NaN       NaN  \n",
       "35836     NaN     NaN     NaN          NaN       NaN  \n",
       "35837     NaN     NaN     NaN          NaN       NaN  \n",
       "35838     NaN     NaN     NaN          NaN       NaN  \n",
       "35839     NaN     NaN     NaN          NaN       NaN  \n",
       "\n",
       "[35840 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import necessary modules due to code execution environment reset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Re-define path and find all CSVs again\n",
    "data_dir = Path(\"/home/ubuntu/fast_llm_inference/RQ3/details\")\n",
    "all_files = list(data_dir.glob(\"*.csv\"))\n",
    "\n",
    "# Filter for *_batch_*.csv files\n",
    "batch_files = [f for f in all_files if \"_batch_\" in f.stem]\n",
    "\n",
    "# Load and annotate each CSV\n",
    "batch_df_list = []\n",
    "for f in batch_files:\n",
    "    parts = f.stem.split(\"_\")\n",
    "    if len(parts) < 4:\n",
    "        continue\n",
    "    engine = parts[0]\n",
    "    model = parts[1]\n",
    "    task = parts[2]\n",
    "    use_case = parts[3]\n",
    "    batch_size = parts[4]\n",
    "\n",
    "    df = pd.read_csv(f)\n",
    "    df[\"engine\"] = engine\n",
    "    df[\"model\"] = model\n",
    "    df[\"task\"] = task\n",
    "    df[\"use_case\"] = use_case\n",
    "    df[\"batch_size\"] = batch_size\n",
    "    batch_df_list.append(df)\n",
    "\n",
    "# Concatenate into one DataFrame\n",
    "merged_batch_df = pd.concat(batch_df_list, ignore_index=True)\n",
    "\n",
    "merged_batch_df.to_csv(\"/home/ubuntu/fast_llm_inference/RQ3/merged_batch_details.csv\", index=False)\n",
    "\n",
    "merged_batch_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d54faf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mistral-7B-Instruct-v0.3', 'Qwen2.5-3B-Instruct',\n",
       "       'Llama-3.1-8B-Instruct'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_batch_df['model'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-generation-server",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

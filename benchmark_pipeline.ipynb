{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_model_handler import LlamaModelHandler\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model: meta-llama/Llama-3.1-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful.\n",
      "Loading model 'meta-llama/Llama-3.1-8b'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:07<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n",
      "GPU: NVIDIA L4\n",
      "Model dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "model_handler = LlamaModelHandler(\"meta-llama/Llama-3.1-8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testprompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Whats the meaning of life? That's a question we all ask at some point in our lives. This is something that has been pondered by many great minds throughout history.\n",
       "But what does it mean to you and me?\n",
       "We are born, live for 80-90 years or so (depending on where we come from) then die and go back into nature.\n",
       "What exactly happens after death though...is anyone really sure?\n",
       "Well here is my take...\n",
       "The idea behind this theory is pretty simple:\n",
       "Our souls have always existed since before birth and will continue existing even when we physically pass away.\n",
       "It could be argued that when one dies their soul goes straight up to heaven to meet with God but I don't believe thats how things work out.\n",
       "Instead Im convinced that your soul continues living through reincarnation; which means being reborn again somewhere else along time lines other than ours - maybe another planet perhaps even Earth itself!\n",
       "There may also exist an infinite number of parallel universes containing identical copies yet completely different versions thereof due certain changes made within each individual instance leading them down separate paths until they eventually become two entirely dissimilar entities once more separated by space & matter\n",
       "In addition there might possibly multiple dimensions beyond those currently known about including ones consisting solely energy instead physical mass"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Whats the meaning of life?\"\n",
    "display(Markdown(model_handler.generate_text(prompt=prompt, max_new_tokens=250)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Performance Benchmarking Metrics**\n",
    "\n",
    "---\n",
    "\n",
    "<small>\n",
    "\n",
    "#### 1. **Latency**\n",
    "\n",
    "Measures time delays during generation.\n",
    "\n",
    "- **First-Token Latency (FTL):**  \n",
    "  Time to generate the **first token**.  \n",
    "  $$ \\text{FTL} = t_{\\text{first token}} - t_{\\text{start}} $$\n",
    "\n",
    "- **Average-Token Latency (ATL):**  \n",
    "  Average time per token after the first one.  \n",
    "  $$ \\text{ATL} = \\frac{T_{\\text{total}} - \\text{FTL}}{N_{\\text{tokens}} - 1} $$\n",
    "\n",
    "- **Generation Latency (GL):**  \n",
    "  Total time to generate the **full output**.  \n",
    "  $$ \\text{GL} = t_{\\text{end}} - t_{\\text{start}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Throughput**\n",
    "\n",
    "Measures the output rate of the model.\n",
    "\n",
    "- **Tokens per Second (TPS):**  \n",
    "  Number of tokens generated per second.  \n",
    "  $$ \\text{TPS} = \\frac{N_{\\text{tokens}}}{\\text{GL}} $$\n",
    "\n",
    "- **Sentences per Second (SPS):**  \n",
    "  Number of sentences generated per second.  \n",
    "  $$ \\text{SPS} = \\frac{N_{\\text{sentences}}}{\\text{GL}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Storage**\n",
    "\n",
    "Provides insights into memory usage during inference.\n",
    "\n",
    "- **Model Size:**  \n",
    "  The total disk space used by the pre-trained model.\n",
    "\n",
    "- **KV-Cache Size:**  \n",
    "  Memory used for key-value caching during generation.\n",
    "\n",
    "- **Memory Usage (Model + KV-Cache):**  \n",
    "  $$ \\text{Memory}_{\\text{total}} = \\text{Model Memory} + \\text{KV-Cache Memory} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Energy**\n",
    "\n",
    "Evaluates energy efficiency during generation.\n",
    "\n",
    "- **Energy Consumption per Token:**  \n",
    "  $$ E_{\\text{token}} = \\frac{E_{\\text{total}}}{N_{\\text{tokens}}} $$\n",
    "\n",
    "- **Energy Consumption per Sentence:**  \n",
    "  $$ E_{\\text{sentence}} = \\frac{E_{\\text{total}}}{N_{\\text{sentences}}} $$\n",
    "\n",
    "- **Energy Consumption per Second:**  \n",
    "  $$ E_{\\text{sec}} = P_{\\text{avg}} \\times t_{\\text{generation}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Quality (Summarization)**\n",
    "\n",
    "Measures the quality of model-generated text, especially for summarization tasks.\n",
    "\n",
    "- **ROUGE Score:**  \n",
    "  Measures the overlap between generated and reference summaries.\n",
    "\n",
    "- **Perplexity:**  \n",
    "  Indicates how well the model predicts a sequence. Lower is better.  \n",
    "  $$ \\text{Perplexity} = e^{\\text{Cross-Entropy Loss}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary of Key Metrics**\n",
    "\n",
    "| Metric                   | Unit             | Formula/Definition                                  |\n",
    "|--------------------------|-------------------|-----------------------------------------------------|\n",
    "| First-Token Latency      | seconds (s)       | $$ \\text{FTL} $$                                    |\n",
    "| Average-Token Latency    | seconds/token     | $$ \\text{ATL} $$                                    |\n",
    "| Generation Latency       | seconds (s)       | $$ \\text{GL} $$                                     |\n",
    "| Tokens per Second (TPS)  | tokens/second     | $$ \\frac{N_{\\text{tokens}}}{\\text{GL}} $$            |\n",
    "| Sentences per Second     | sentences/second  | $$ \\frac{N_{\\text{sentences}}}{\\text{GL}} $$         |\n",
    "| Memory Usage             | MB/GB             | $$ \\text{Model Memory} + \\text{KV-Cache Memory} $$   |\n",
    "| Energy per Token         | Joules/token      | $$ \\frac{E_{\\text{total}}}{N_{\\text{tokens}}} $$     |\n",
    "| Energy per Sentence      | Joules/sentence   | $$ \\frac{E_{\\text{total}}}{N_{\\text{sentences}}} $$  |\n",
    "| Energy per Second        | Watts (W)         | $$ P_{\\text{avg}} \\times t_{\\text{generation}} $$    |\n",
    "| Perplexity               | -                 | $$ e^{\\text{Cross-Entropy Loss}} $$                  |\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark import ModelBenchmark\n",
    "from llama_model_handler import LlamaModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful.\n",
      "Loading model 'meta-llama/Llama-3.1-8b' with precision 'fp16'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1c9e82fd89447380a4affaa6af0563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n",
      "GPU: NVIDIA L4\n",
      "Model dtype: torch.float16\n",
      "Model loading time: 11.5443 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_handler = LlamaModelHandler(\"meta-llama/Llama-3.1-8b\", precision=\"fp16\")\n",
    "model, tokenizer = model_handler.get_model_and_tokenizer()\n",
    "\n",
    "# Initialize benchmark\n",
    "benchmark = ModelBenchmark(model=model, tokenizer=tokenizer, max_tokens=128)\n",
    "\n",
    "# Run benchmark\n",
    "test_prompts = [\n",
    "    \"Explain the significance of transformer models in NLP.\",\n",
    "    \"What are the main benefits of renewable energy?\",\n",
    "    \"How does the immune system work?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the best way to cook a steak?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 54 characters)...\n",
      "Power logging started for prompt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power logging stopped for prompt.\n",
      "Evaluating prompt (length 47 characters)...\n",
      "Power logging started for prompt.\n",
      "Power logging stopped for prompt.\n",
      "Evaluating prompt (length 32 characters)...\n",
      "Power logging started for prompt.\n",
      "Power logging stopped for prompt.\n",
      "Evaluating prompt (length 30 characters)...\n",
      "Power logging started for prompt.\n",
      "Power logging stopped for prompt.\n",
      "Evaluating prompt (length 37 characters)...\n",
      "Power logging started for prompt.\n",
      "Power logging stopped for prompt.\n"
     ]
    }
   ],
   "source": [
    "benchmark_results_fp16 = benchmark.benchmark(test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Length</th>\n",
       "      <th>FTL (s)</th>\n",
       "      <th>ATL (s)</th>\n",
       "      <th>GL (s)</th>\n",
       "      <th>TPS (tokens/s)</th>\n",
       "      <th>SPS (sentences/s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Total Energy Consumption (Wh)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>9.0870</td>\n",
       "      <td>17.06</td>\n",
       "      <td>0.73</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>0.329708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>8.2516</td>\n",
       "      <td>16.82</td>\n",
       "      <td>0.98</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>0.326103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0603</td>\n",
       "      <td>0.0603</td>\n",
       "      <td>8.2070</td>\n",
       "      <td>16.56</td>\n",
       "      <td>0.73</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>0.327567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>8.2194</td>\n",
       "      <td>16.54</td>\n",
       "      <td>1.82</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>0.326733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>8.2309</td>\n",
       "      <td>16.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>0.326532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prompt Length  FTL (s)  ATL (s)  GL (s)  TPS (tokens/s)  SPS (sentences/s)  \\\n",
       "0             54   0.0649   0.0649  9.0870           17.06               0.73   \n",
       "1             47   0.0598   0.0598  8.2516           16.82               0.98   \n",
       "2             32   0.0603   0.0603  8.2070           16.56               0.73   \n",
       "3             30   0.0604   0.0604  8.2194           16.54               1.82   \n",
       "4             37   0.0592   0.0592  8.2309           16.90               0.00   \n",
       "\n",
       "   Memory Usage (MB)  Total Energy Consumption (Wh)  \n",
       "0           16190.06                       0.329708  \n",
       "1           16190.06                       0.326103  \n",
       "2           16190.06                       0.327567  \n",
       "3           16190.06                       0.326733  \n",
       "4           16190.06                       0.326532  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_results_fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of 8bit quantization reduced the memory usage as expeted. Interesting is, that the energy consumption per token is higher than the one of the 16bit model. 8bit quantization is slower than 16bit quantization which leads to the higher energy consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Length</th>\n",
       "      <th>FTL (s)</th>\n",
       "      <th>ATL (s)</th>\n",
       "      <th>GL (s)</th>\n",
       "      <th>TPS (tokens/s)</th>\n",
       "      <th>SPS (sentences/s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Total Energy Consumption (Wh)</th>\n",
       "      <th>GL (s)/Energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>13.8477</td>\n",
       "      <td>11.67</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9462.06</td>\n",
       "      <td>0.373899</td>\n",
       "      <td>37.035938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>0.0861</td>\n",
       "      <td>0.0861</td>\n",
       "      <td>11.8813</td>\n",
       "      <td>11.34</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9462.06</td>\n",
       "      <td>0.371903</td>\n",
       "      <td>31.947309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>11.7006</td>\n",
       "      <td>11.54</td>\n",
       "      <td>0.42</td>\n",
       "      <td>9462.06</td>\n",
       "      <td>0.371951</td>\n",
       "      <td>31.457369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>11.9751</td>\n",
       "      <td>11.47</td>\n",
       "      <td>1.29</td>\n",
       "      <td>9462.06</td>\n",
       "      <td>0.332548</td>\n",
       "      <td>36.010140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>11.5187</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9462.06</td>\n",
       "      <td>0.371158</td>\n",
       "      <td>31.034492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prompt Length  FTL (s)  ATL (s)   GL (s)  TPS (tokens/s)  \\\n",
       "0             54   0.0989   0.0989  13.8477           11.67   \n",
       "1             47   0.0861   0.0861  11.8813           11.34   \n",
       "2             32   0.0860   0.0860  11.7006           11.54   \n",
       "3             30   0.0881   0.0881  11.9751           11.47   \n",
       "4             37   0.0829   0.0829  11.5187           11.93   \n",
       "\n",
       "   SPS (sentences/s)  Memory Usage (MB)  Total Energy Consumption (Wh)  \\\n",
       "0               0.50            9462.06                       0.373899   \n",
       "1               0.49            9462.06                       0.371903   \n",
       "2               0.42            9462.06                       0.371951   \n",
       "3               1.29            9462.06                       0.332548   \n",
       "4               0.52            9462.06                       0.371158   \n",
       "\n",
       "   GL (s)/Energy  \n",
       "0      37.035938  \n",
       "1      31.947309  \n",
       "2      31.457369  \n",
       "3      36.010140  \n",
       "4      31.034492  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_results_8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/llmperf_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-26 07:32:41,682\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Using local LLaMA model for benchmarking.\n",
      "Authentication successful.\n",
      "Loading model 'meta-llama/Llama-3.1-8b' with precision 'fp16'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:08<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n",
      "GPU: NVIDIA L4\n",
      "Model dtype: torch.float16\n",
      "Model loading time: 10.6879 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:43<00:00,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatched and errored requests.\n",
      "    mismatched request: Convert the following sequence of words into a number: eight thousand and forty-six.\\nPrint the number first. Then print its digits in reverse order.\n",
      "```\n",
      "#include <stdio.h>\n",
      "int main()\n",
      "{\n",
      "    int num = 8046, r;\n",
      "     printf(\"%d\",num);\n",
      "     while(num!=0)\n",
      "      {\n",
      "        r=num%10;  \n",
      "          //print each digit\n",
      "            if (r==1) \n",
      "                {printf(\"one \");}\n",
      "            else if(r ==2){printf(\"two \");}    \n",
      "             else if ( r==3){printf (\"three\");}\n",
      "              else if, expected: 8046\n",
      "    mismatched request: Convert the following sequence of words into a number: eight thousand and forty-six.\\nPrint the number first. Then print your answer in terms of digits.\n",
      "8*1000+4*10^2 + 6 =80406\n",
      "i.e. Eighty four zero six is equal to eight hundred forty six, expected: 8046\n",
      "    mismatched request: Convert the following sequence of words into a number: three thousand, one hundred and thirty-two.\\nPrint the number first. Then convert it to its expanded form.\n",
      "The program should use a class named WordConverter that is capable of converting numbers from their written (expanded) format to an integer value.\n",
      "\n",
      "``class   WordConverter :\n",
      "     def   __init__ ( self,  word ):\n",
      "         self. _word   =   str ( word )\n",
      "         self._value  =  None\n",
      "         \n",
      "     @property\n",
      "     def  value( self ): \n",
      "          return   int(self._value)\n",
      "          \n",
      "      # TODO:, expected: 3132\n",
      "    mismatched request: Convert the following sequence of words into a number: three thousand, one hundred and thirty-two.\\nPrint the number first. Then convert it back to its word form.\n",
      "3. Use only numbers for your answer (don‚Äôt use any punctuation marks).\n",
      "4. If you‚Äôre in doubt about what format or style should be used for this task just write whatever comes to mind!\n",
      "5) How would someone else interpret these instructions? What might they think we want them do when given such vague directions like ‚Äúconvert‚Äù without specifying how exactly something needs converting from one thing into another?\n",
      "6) Are there other ways besides math equations that, expected: 3132\n",
      "    mismatched request: Convert the following sequence of words into a number: eight thousand, nine hundred and ninety-six.\\nPrint the number first. Then print its digits in reverse order.\n",
      "The task is to convert the given string representation of numbers (with commas) into an integer value using recursion.\n",
      "Let‚Äôs say we have two functions:\n",
      "int getNumber(string s)\n",
      "string getDigits(int n)\n",
      "s = ‚Äúeight thousand, five hundred and twenty-three‚Äù\n",
      "Now let us call these functions recursively as shown below\n",
      "1.) Call function ‚ÄògetNumber‚Äô with input argument ‚Äòs‚Äô\n",
      "2.) Now suppose that our recursive call returns 8532 for, expected: 8996\n",
      "    mismatched request: Convert the following sequence of words into a number: eight thousand, nine hundred and ninety-six.\\nPrint the number first. \\nThen print in word form.\n",
      "Hint- Here we are given to convert this sentence in numerical form i.e., 9896 in numbers then again from that numeral value it should be converted back into its original form as ‚Äúeight thousand, five hundred and twenty-five‚Äù in text format.\n",
      "As per question here we need to find out the total sum of all digits which is written there like $8+9+9=26$.\n",
      "Now after getting the total count for the above mentioned number, expected: 8996\n",
      "    mismatched request: Convert the following sequence of words into a number: nine thousand, six hundred and eighty-one.\\nPrint the number first. Then convert it back to its word form.\n",
      ", expected: 9681\n",
      "    mismatched request: Convert the following sequence of words into a number: nine thousand, six hundred and eighty-one.\\nPrint the number first. Then print it in digits.\n",
      "\n",
      "# Code\n",
      "\n",
      "```python\n",
      "def word_to_number(word):\n",
      "    # Define dictionary for mapping each digit with its corresponding string.\n",
      "    dic = {\n",
      "        'one': 1,\n",
      "        'two': 2,\n",
      "        \"three\": 3,\n",
      "       ...\n",
      "        }\n",
      "    \n",
      "    num_list = []\n",
      "    \n",
      "    if len(word) == 0:\n",
      "        \n",
      "        return None\n",
      "    \n",
      "    elif (len(word)) < 5:\n",
      "        \n",
      "            l_word = list(word.split(', expected: 9681\n",
      "    mismatched request: Convert the following sequence of words into a number: seven thousand, five hundred and eighty-eight.\\nPrint the number first. Then print it in binary format.\n",
      ", expected: 7588\n",
      "    mismatched request: Convert the following sequence of words into a number: seven thousand, five hundred and eighty-eight.\\nPrint the number first. Then print its representation in English.\n",
      ", expected: 7588\n",
      "Benchmark Results: {'number_errors': 0, 'num_mismatched_requests': 10, 'error_rate': 0.0, 'mismatch_rate': 1.0, 'num_completed_requests': 10, 'num_non_errored_requests': 10, 'model': LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "), 'num_concurrent_requests': 2, 'additional_sampling_params': {}, 'llm_api': 'local'}\n",
      "Details: [{'metrics': {}, 'generated_text': 'Convert the following sequence of words into a number: eight thousand and forty-six.\\\\nPrint the number first. Then print its digits in reverse order.\\n```\\n#include <stdio.h>\\nint main()\\n{\\n    int num = 8046, r;\\n     printf(\"%d\",num);\\n     while(num!=0)\\n      {\\n        r=num%10;  \\n          //print each digit\\n            if (r==1) \\n                {printf(\"one \");}\\n            else if(r ==2){printf(\"two \");}    \\n             else if ( r==3){printf (\"three\");}\\n              else if', 'request_config': {'rnd_number': 8046}}, {'metrics': {}, 'generated_text': 'Convert the following sequence of words into a number: eight thousand and forty-six.\\\\nPrint the number first. Then print your answer in terms of digits.\\n8*1000+4*10^2 + 6 =80406\\ni.e. Eighty four zero six is equal to eight hundred forty six', 'request_config': {'rnd_number': 8046}}, {'metrics': {}, 'generated_text': 'Convert the following sequence of words into a number: three thousand, one hundred and thirty-two.\\\\nPrint the number first. Then convert it to its expanded form.\\nThe program should use a class named WordConverter that is capable of converting numbers from their written (expanded) format to an integer value.\\n\\n``class   WordConverter :\\n     def   __init__ ( self,  word ):\\n         self. _word   =   str ( word )\\n         self._value  =  None\\n         \\n     @property\\n     def  value( self ): \\n          return   int(self._value)\\n          \\n      # TODO:', 'request_config': {'rnd_number': 3132}}, {'metrics': {}, 'generated_text': 'Convert the following sequence of words into a number: three thousand, one hundred and thirty-two.\\\\nPrint the number first. Then convert it back to its word form.\\n3. Use only numbers for your answer (don‚Äôt use any punctuation marks).\\n4. If you‚Äôre in doubt about what format or style should be used for this task just write whatever comes to mind!\\n5) How would someone else interpret these instructions? What might they think we want them do when given such vague directions like ‚Äúconvert‚Äù without specifying how exactly something needs converting from one thing into another?\\n6) Are there other ways besides math equations that', 'request_config': {'rnd_number': 3132}}, {'metrics': {}, 'generated_text': 'Convert the following sequence of words into a number: eight thousand, nine hundred and ninety-six.\\\\nPrint the number first. Then print its digits in reverse order.\\nThe task is to convert the given string representation of numbers (with commas) into an integer value using recursion.\\nLet‚Äôs say we have two functions:\\nint getNumber(string s)\\nstring getDigits(int n)\\ns = ‚Äúeight thousand, five hundred and twenty-three‚Äù\\nNow let us call these functions recursively as shown below\\n1.) Call function ‚ÄògetNumber‚Äô with input argument ‚Äòs‚Äô\\n2.) Now suppose that our recursive call returns 8532 for', 'request_config': {'rnd_number': 8996}}, {'metrics': {}, 'generated_text': 'Convert the following sequence of words into a number: eight thousand, nine hundred and ninety-six.\\\\nPrint the number first. \\\\nThen print in word form.\\nHint- Here we are given to convert this sentence in numerical form i.e., 9896 in numbers then again from that numeral value it should be converted back into its original form as ‚Äúeight thousand, five hundred and twenty-five‚Äù in text format.\\nAs per question here we need to find out the total sum of all digits which is written there like $8+9+9=26$.\\nNow after getting the total count for the above mentioned number', 'request_config': {'rnd_number': 8996}}, {'metrics': {}, 'generated_text': 'Convert the following sequence of words into a number: nine thousand, six hundred and eighty-one.\\\\nPrint the number first. Then convert it back to its word form.\\n', 'request_config': {'rnd_number': 9681}}, {'metrics': {}, 'generated_text': 'Convert the following sequence of words into a number: nine thousand, six hundred and eighty-one.\\\\nPrint the number first. Then print it in digits.\\n\\n# Code\\n\\n```python\\ndef word_to_number(word):\\n    # Define dictionary for mapping each digit with its corresponding string.\\n    dic = {\\n        \\'one\\': 1,\\n        \\'two\\': 2,\\n        \"three\": 3,\\n       ...\\n        }\\n    \\n    num_list = []\\n    \\n    if len(word) == 0:\\n        \\n        return None\\n    \\n    elif (len(word)) < 5:\\n        \\n            l_word = list(word.split(\\'', 'request_config': {'rnd_number': 9681}}, {'metrics': {}, 'generated_text': 'Convert the following sequence of words into a number: seven thousand, five hundred and eighty-eight.\\\\nPrint the number first. Then print it in binary format.\\n', 'request_config': {'rnd_number': 7588}}, {'metrics': {}, 'generated_text': 'Convert the following sequence of words into a number: seven thousand, five hundred and eighty-eight.\\\\nPrint the number first. Then print its representation in English.\\n', 'request_config': {'rnd_number': 7588}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add LLMPerf path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', 'llmperf'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from llm_correctness import llm_correctness\n",
    "\n",
    "# Run the benchmark using your local model\n",
    "results, details = llm_correctness(\n",
    "    model=\"meta-llama/Llama-3.1-8b\",\n",
    "    llm_api=\"local\",  # üëà Use the newly added local option\n",
    "    num_concurrent_requests=2,\n",
    "    max_num_completed_requests=10\n",
    ")\n",
    "\n",
    "print(\"Benchmark Results:\", results)\n",
    "print(\"Details:\", details)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

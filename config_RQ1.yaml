experiment_name: RQ1

backend: vllm

hf_model:
  # Qwen2.5
  - Qwen/Qwen2.5-1.5B-Instruct
  - Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4
  - Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int8

  - Qwen/Qwen2.5-3B-Instruct
  - Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4
  - Qwen/Qwen2.5-3B-Instruct-GPTQ-Int8

  - Qwen/Qwen2.5-7B-Instruct
  - Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4
  - Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8

  - Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4
  - Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8

  - Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4

  # Mistral
  - mistralai/Mistral-7B-Instruct-v0.3
  - marinarosell/Mistral-7B-Instruct-v0.3-GPTQ-8bit-gs128
  - marinarosell/Mistral-7B-Instruct-v0.3-GPTQ-4bit-gs128

  - dwetzel/Mistral-Small-24B-Instruct-2501-GPTQ-INT4

  # Gemma-2
  - google/gemma-2-2b-it
  - marcinbrzezanski/gemma-2-2b-it-gptq
  - RedHatAI/gemma-2-2b-it-quantized.w8a16

  - google/gemma-2-9b-it
  - shuyuej/gemma-2-9b-it-GPTQ
  - RedHatAI/gemma-2-9b-it-quantized.w8a16

  - shuyuej/gemma-2-27b-it-GPTQ

  # LLaMA 3.x
  - meta-llama/Llama-3.2-1B-Instruct
  - clowman/Llama-3.2-1B-Instruct-GPTQ-Int4
  - clowman/Llama-3.2-1B-Instruct-GPTQ-Int8

  - meta-llama/Llama-3.2-3B-Instruct
  - clowman/Llama-3.2-3B-Instruct-GPTQ-Int8
  - clowman/Llama-3.2-3B-Instruct-GPTQ-Int4

  - meta-llama/Llama-3.1-8B-Instruct
  - clowman/Llama-3.1-8B-Instruct-GPTQ-Int4
  - clowman/Llama-3.1-8B-Instruct-GPTQ-Int8

task:      [summarization, sql, qa]
scenario:  [batch]

samples:          256
sample_interval:  0.1
quality_metric:   true

batch_size: [16]

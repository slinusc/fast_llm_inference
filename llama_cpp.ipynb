{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 15:04:30.440395: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743779070.459804  158193 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743779070.467218  158193 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743779070.488881  158193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743779070.488897  158193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743779070.488900  158193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743779070.488902  158193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-04 15:04:30.494487: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA L4, compute capability 8.9, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU offload supported: True\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU offload supported:\", llama_cpp.llama_supports_gpu_offload())\n",
    "\n",
    "# Path to your GGUF model (adjust path if needed)\n",
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/llama-3.1-8B-Instruct-f16.gguf\"\n",
    "\n",
    "# Initialize Llama with GPU layers offloaded\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=-1,     # Offload to GPU! Adjust as needed based on your VRAM\n",
    "    verbose=False         # Prints backend info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "dataset = load_dataset(\"gigaword\", split=\"test[:100]\")  # Limit to 100 for fast eval\n",
    "\n",
    "# Initialize ROUGE metric\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'France names unchanged team for second test'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_news_few_shot(document, max_tokens=20):\n",
    "    \"\"\"\n",
    "    Summarize the given `document` into a concise headline using a few-shot prompt.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a headline generation assistant. Given a news article, produce a concise and informative headline.\\n\\n\"\n",
    "\n",
    "        \"Here is an example:\\n\"\n",
    "        \"News: Japan's NEC Corp. and UNK Computer Corp. of the United States said Wednesday they had agreed to join forces in supercomputer sales.\\n\"\n",
    "        \"Headline: NEC and UNK in supercomputer sales deal\\n\\n\"\n",
    "\n",
    "        \"News: Scientists have discovered a new exoplanet that appears to have water on its surface, raising hopes it may be habitable.\\n\"\n",
    "        \"Headline: New exoplanet may support life\\n\\n\"\n",
    "\n",
    "        f\"News: {document}\\n\"\n",
    "        \"Headline:\"\n",
    "    )\n",
    "\n",
    "    response = llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.2,\n",
    "        top_p=1.0,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    return response[\"choices\"][0][\"text\"].strip().split(\"\\n\")[0]\n",
    "\n",
    "\n",
    "summarize_news_few_shot(dataset[19][\"document\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sri lanka closes schools as war escalates'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1][\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for item in dataset:\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_news_few_shot(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "df = pd.DataFrame({\n",
    "    'Reference': references,\n",
    "    'Prediction': predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nec UNK in computer sales tie-up</td>\n",
       "      <td>NEC and UNK in supercomputer sales deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sri lanka closes schools as war escalates</td>\n",
       "      <td>Sri Lanka closes schools amid escalating conflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>protesters target french research ship</td>\n",
       "      <td>Five arrested in anti-nuclear protest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us september factory orders up #.# percent</td>\n",
       "      <td>US factory orders rise in September</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bank of UNK UNK for calm in financial markets</td>\n",
       "      <td>Japan urges calm after US orders Daiwa Bank cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>notre dame cathedral square to be named after ...</td>\n",
       "      <td>Notre Dame square to be renamed after Pope Joh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>somali warlords stronghold tense after us-back...</td>\n",
       "      <td>Somali town prepares for clashes as warlords flee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>press lambasts sorry french display</td>\n",
       "      <td>France's World Cup opener ends in draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>algerian press freedom at risk despite editor ...</td>\n",
       "      <td>Algeria's press freedom still at risk despite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>nalbandian optimistic for wimbledon fitness</td>\n",
       "      <td>Nalbandian optimistic for Wimbledon return</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Reference  \\\n",
       "0                    nec UNK in computer sales tie-up   \n",
       "1           sri lanka closes schools as war escalates   \n",
       "2              protesters target french research ship   \n",
       "3          us september factory orders up #.# percent   \n",
       "4       bank of UNK UNK for calm in financial markets   \n",
       "..                                                ...   \n",
       "95  notre dame cathedral square to be named after ...   \n",
       "96  somali warlords stronghold tense after us-back...   \n",
       "97                press lambasts sorry french display   \n",
       "98  algerian press freedom at risk despite editor ...   \n",
       "99        nalbandian optimistic for wimbledon fitness   \n",
       "\n",
       "                                           Prediction  \n",
       "0             NEC and UNK in supercomputer sales deal  \n",
       "1   Sri Lanka closes schools amid escalating conflict  \n",
       "2               Five arrested in anti-nuclear protest  \n",
       "3                 US factory orders rise in September  \n",
       "4   Japan urges calm after US orders Daiwa Bank cl...  \n",
       "..                                                ...  \n",
       "95  Notre Dame square to be renamed after Pope Joh...  \n",
       "96  Somali town prepares for clashes as warlords flee  \n",
       "97             France's World Cup opener ends in draw  \n",
       "98  Algeria's press freedom still at risk despite ...  \n",
       "99         Nalbandian optimistic for Wimbledon return  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### float16 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B) float16 Summarization Results:\n",
      "\n",
      "Number of examples: 100\n",
      "\n",
      "Elapsed time: 124.30 s\n",
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.3443\n",
      "rouge2: 0.1337\n",
      "rougeL: 0.3257\n",
      "rougeLsum: 0.3290\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"llama.cpp (Llama-3.1-8B) float16 Summarization Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(references)}\")\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now trying the 8bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,         # 8K tokens context is fine unless you need more\n",
    "    n_gpu_layers=-1,    # L4 GPU 24 GB can usually handle **60-80 layers** for 8B models\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for item in dataset:\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_news_few_shot(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B) 8Bit Summarization Results:\n",
      "\n",
      "Number of examples: 100\n",
      "\n",
      "Elapsed time: 72.05 s\n",
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.3551\n",
      "rouge2: 0.1375\n",
      "rougeL: 0.3360\n",
      "rougeLsum: 0.3359\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"llama.cpp (Llama-3.1-8B) 8Bit Summarization Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(references)}\")\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now 4Bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,         # 8K tokens context is fine unless you need more\n",
    "    n_gpu_layers=80,    # L4 GPU 24 GB can usually handle **60-80 layers** for 8B models\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for item in dataset:\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_news_few_shot(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B) 4Bit Summarization Results:\n",
      "\n",
      "Number of examples: 100\n",
      "\n",
      "Elapsed time: 46.13 s\n",
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.3342\n",
      "rouge2: 0.1195\n",
      "rougeL: 0.3140\n",
      "rougeLsum: 0.3130\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"llama.cpp (Llama-3.1-8B) 4Bit Summarization Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(references)}\")\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA 3.1 8B Summarization Benchmark\n",
    "\n",
    "<small>\n",
    "\n",
    "#### üìù Tested Models\n",
    "| **Model File**                      | **Precision** | **Quantization Scheme** | **Notes**                                     |\n",
    "|-------------------------------------|---------------|-------------------------|-----------------------------------------------|\n",
    "| llama-3.1-8B-f16.gguf               | float16       | Full Precision          | Largest model, highest theoretical accuracy, slowest inference |\n",
    "| llama-3.1-8B-Q8_0.gguf              | 8-bit         | Q8_0                   | Reduced size, good balance of speed and quality |\n",
    "| llama-3.1-8B-Q4_K_M.gguf            | 4-bit         | Q4_K_M                 | Highly optimized 4-bit quantization, best summarization quality in tests |\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä Summarization Results\n",
    "\n",
    "| **Platform / Model**               | **Elapsed Time (s)** | **ROUGE-1** | **ROUGE-2** | **ROUGE-L** | **ROUGE-Lsum** |\n",
    "|------------------------------------|----------------------|-------------|-------------|-------------|----------------|\n",
    "| Ollama (LLaMA 3.1 8B Q4_K_M)       | 49.06                | 0.2886      | 0.1040      | 0.2632      | 0.2658         |\n",
    "| llama.cpp (Q4_K_M)                 | 64.42                | 0.2699      | 0.1160      | 0.2491      | 0.2501         |\n",
    "| llama.cpp (Q8_0)                   | 161.84               | 0.1788      | 0.0608      | 0.1580      | 0.1631         |\n",
    "| llama.cpp (float16)                | 161.65               | 0.1801      | 0.0599      | 0.1591      | 0.1629         |\n",
    "\n",
    "---\n",
    "\n",
    "#### üîç Summary of Insights\n",
    "\n",
    "- **4-bit quantized models (Q4_K_M)** in both **Ollama** and **llama.cpp** delivered **better summarization quality** and **faster inference** than higher-precision models.\n",
    "- The **Q4_K_M quantization scheme** preserves summarization performance surprisingly well and matches Ollama's results.\n",
    "- **8-bit (Q8_0)** and **float16** models performed worse in ROUGE scores, despite having more precision. This may be due to:\n",
    "  - Differences in **prompt formatting**\n",
    "  - **Sampling parameters**\n",
    "  - Potential model variant differences (instruction-tuned vs base models)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Recommendations\n",
    "\n",
    "1. Use **Q4_K_M quantized models** in llama.cpp for comparable performance to Ollama.\n",
    "2. Match **prompt templates** used in Ollama:\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Q4_K_M vs Q8_0 Quantization Comparison\n",
    "\n",
    "<small>\n",
    "\n",
    "#### What Are They?\n",
    "Both **Q4_K_M** and **Q8_0** are quantization methods used to compress model weights for faster inference and lower memory usage.\n",
    "\n",
    "---\n",
    "\n",
    "#### Q4_K_M (4-bit Quantization, Optimized)\n",
    "\n",
    "| Feature          | Description |\n",
    "|------------------|-------------|\n",
    "| **Precision**    | 4-bit |\n",
    "| **Quantization Type** | \"K\" series, specifically **K_M** (multi-purpose optimized) |\n",
    "| **Compression**  | Very high (significantly smaller than 8-bit) |\n",
    "| **Speed**        | Extremely fast, ideal for CPU/GPU |\n",
    "| **Memory Usage** | Very low (fits on smaller GPUs like 6-8GB VRAM) |\n",
    "| **Accuracy**     | Preserves high accuracy in **instruction-tuned tasks** like **summarization**, **chat**, and **QA** |\n",
    "| **Best Use Cases** | Chatbots, summarization, reasoning tasks |\n",
    "| **Notes**        | Uses **groupwise quantization** and **per-channel scaling** for better accuracy retention despite low precision |\n",
    "\n",
    "---\n",
    "\n",
    "#### Q8_0 (8-bit Quantization, General Purpose)\n",
    "\n",
    "| Feature          | Description |\n",
    "|------------------|-------------|\n",
    "| **Precision**    | 8-bit |\n",
    "| **Quantization Type** | Uniform 8-bit |\n",
    "| **Compression**  | Moderate (smaller than float16 but larger than 4-bit) |\n",
    "| **Speed**        | Faster than float16, but slower than Q4_K_M |\n",
    "| **Memory Usage** | Moderate (needs more VRAM, typically 12GB+) |\n",
    "| **Accuracy**     | Higher precision retention in general, but not optimized for specific tasks |\n",
    "| **Best Use Cases** | Complex reasoning, precision-sensitive tasks |\n",
    "| **Notes**        | General-purpose quantization without task-specific optimizations |\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öñÔ∏è Comparison Table: Q4_K_M vs Q8_0\n",
    "\n",
    "| Feature          | **Q4_K_M**                  | **Q8_0**                  |\n",
    "|------------------|-----------------------------|---------------------------|\n",
    "| **Precision**    | 4-bit                       | 8-bit                    |\n",
    "| **Size**         | Very small                  | Medium                   |\n",
    "| **Speed**        | Very fast (low latency)     | Fast (higher latency than 4-bit) |\n",
    "| **VRAM/Memory**  | Very low usage (fits on smaller GPUs/CPUs) | Medium (requires more VRAM) |\n",
    "| **Accuracy**     | High for summarization, chat, reasoning (optimized quantization) | General higher precision (not task-optimized) |\n",
    "| **Task Tuning**  | Task-specific optimizations (instruction-following, summarization) | General-purpose |\n",
    "| **Best Use**     | Chatbots, summarization, QA tasks with constrained resources | Complex reasoning or precision-sensitive tasks |\n",
    "| **Ollama Default?** | ‚úÖ Frequently used (Q4_K_M or Q4_K_S) | ‚ùå Usually not used |\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Why Q4_K_M Outperformed Q8_0 in Summarization\n",
    "- **Q4_K_M** is optimized for **task-specific performance**, often giving better results for **instruction-tuned models**, **summarization**, and **chat** tasks.\n",
    "- **Q8_0** retains more raw precision but isn't tuned for these tasks, leading to lower scores in ROUGE evaluation.\n",
    "- **Q4_K_M** also runs significantly faster with less resource usage.\n",
    "\n",
    "---\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† How Q4_K_M Is Optimized\n",
    "\n",
    "<small>\n",
    "\n",
    "Q4_K_M is part of the advanced **K series** quantization schemes, designed to balance **speed**, **size**, and **accuracy**. It introduces several optimizations to maintain high task performance despite being a 4-bit quantization.\n",
    "\n",
    "#### Key Optimizations\n",
    "\n",
    "| Optimization                   | Description |\n",
    "|--------------------------------|-------------|\n",
    "| **Groupwise Quantization**     | Weights are divided into small groups (e.g., 32 or 64) and quantized individually, improving precision retention. |\n",
    "| **Per-Channel Scaling**        | Each group or channel has its own scale factor, ensuring finer control over the quantization process. |\n",
    "| **Mixed Weight Packing (M)**   | Uses different packing strategies optimized for different layers (e.g., attention vs MLP layers). |\n",
    "| **Dynamic Zero Points**        | Zero points are dynamically computed within groups, reducing quantization bias. |\n",
    "| **Efficient SIMD Utilization** | The packed format is optimized for vectorized operations on CPU and GPU, increasing inference speed. |\n",
    "\n",
    "#### Why It Works Well\n",
    "- Optimized for **instruction-following**, **summarization**, and **chat** tasks.\n",
    "- Preserves task-critical accuracy despite aggressive compression.\n",
    "- Runs **very efficiently** on both CPU and GPU.\n",
    "\n",
    "#### Q4_K_M vs Q8_0\n",
    "\n",
    "| Feature            | Q4_K_M                   | Q8_0                 |\n",
    "|--------------------|--------------------------|----------------------|\n",
    "| Precision          | 4-bit                    | 8-bit               |\n",
    "| Compression        | High                     | Medium              |\n",
    "| Accuracy Retention | High (task-optimized)    | High (general)      |\n",
    "| Speed              | Very fast                | Fast                |\n",
    "| Memory Usage       | Very low                 | Medium              |\n",
    "| Task Tuning        | Summarization, Chat, QA  | General-purpose     |\n",
    "| Ollama Use         | ‚úÖ Often used (default)  | ‚ùå Less common       |\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Load SQuAD v2 dataset (validation split)\n",
    "squad_v2 = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Convert the validation split to a list and sample 200 random questions\n",
    "validation_list = list(squad_v2[\"validation\"])\n",
    "sampled_questions = random.sample(validation_list, 200)\n",
    "\n",
    "questions_with_answers = [i for i in sampled_questions if len(i['answers']['text']) > 0]\n",
    "\n",
    "len(questions_with_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 07:52:16.539537: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742025136.557367  403701 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742025136.562764  403701 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742025136.577713  403701 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742025136.577729  403701 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742025136.577731  403701 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742025136.577733  403701 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-15 07:52:16.582555: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from evaluate import load\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import time\n",
    "from evaluate import load\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles, and extra whitespace.\"\"\"\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punctuation(lower(s))))\n",
    "\n",
    "def clean_prediction(prediction):\n",
    "    \"\"\"\n",
    "    Cleans the raw prediction output from llama.cpp.\n",
    "    - Truncates at a new line, 'Context:', or other stop signals.\n",
    "    - Normalizes the prediction.\n",
    "    \"\"\"\n",
    "    # Split on common stop sequences\n",
    "    stop_tokens = [\"\\n\\n\", \"\\nContext:\", \"Context:\", \"Question:\"]\n",
    "    for stop in stop_tokens:\n",
    "        if stop in prediction:\n",
    "            prediction = prediction.split(stop)[0]\n",
    "\n",
    "    return normalize_answer(prediction)\n",
    "\n",
    "\n",
    "def compute_exact_match(prediction, ground_truths):\n",
    "    \"\"\"Exact match: 1 if prediction is in ground_truths, else 0.\"\"\"\n",
    "    return int(prediction in ground_truths)\n",
    "\n",
    "def compute_f1(prediction, ground_truths):\n",
    "    \"\"\"Compute the maximum F1 over all ground truths.\"\"\"\n",
    "    def get_tokens(s):\n",
    "        return normalize_answer(s).split()\n",
    "\n",
    "    pred_tokens = get_tokens(prediction)\n",
    "    if not pred_tokens:\n",
    "        return int(not any(get_tokens(gt) for gt in ground_truths))\n",
    "\n",
    "    scores = []\n",
    "    for gt in ground_truths:\n",
    "        gt_tokens = get_tokens(gt)\n",
    "        common = set(pred_tokens) & set(gt_tokens)\n",
    "        num_same = len(common)\n",
    "\n",
    "        if num_same == 0:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "\n",
    "        precision = num_same / len(pred_tokens)\n",
    "        recall = num_same / len(gt_tokens)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        scores.append(f1)\n",
    "\n",
    "    return max(scores)\n",
    "\n",
    "\n",
    "def qa_with_llama_cpp(example, max_tokens=50, verbose=True):\n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "    ground_truth_answers = example['answers']\n",
    "\n",
    "    prompt_template = (\n",
    "        \"You are a question answering assistant. Given the context, answer the question. \"\n",
    "        \"If the answer isn't in the context, say 'I don't know'.\\n\\n\"\n",
    "\n",
    "        \"Here is an example:\\n\"\n",
    "        \"Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni)...\\n\"\n",
    "        \"Question: What is the name of the region the Normans gave their name to?\\n\"\n",
    "        \"Answer: Normandy\\n\\n\"\n",
    "\n",
    "        \"Context: {context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    prompt = prompt_template.format(context=context, question=question)\n",
    "\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.0,\n",
    "        \"stream\": False,\n",
    "        \"seed\": 0,\n",
    "    }\n",
    "\n",
    "    response = llm(**payload)\n",
    "    raw_prediction = response['choices'][0]['text'].strip()\n",
    "\n",
    "    # Clean and normalize the prediction\n",
    "    prediction = clean_prediction(raw_prediction)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truths\": ground_truth_answers['text'],\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_qa_with_llama_cpp(dataset, qa_function, save_path=None, skip_unanswerable=True):\n",
    "    squad_metric = load(\"squad\")\n",
    "    references = []\n",
    "    predictions = []\n",
    "    per_example_results = []\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for example in dataset:\n",
    "        if skip_unanswerable and len(example['answers']['text']) == 0:\n",
    "            continue\n",
    "\n",
    "        result = qa_function(example)\n",
    "\n",
    "        if result['prediction'] is not None:\n",
    "            normalized_prediction = normalize_answer(result['prediction'])\n",
    "            normalized_ground_truths = [normalize_answer(ans) for ans in result['ground_truths']]\n",
    "\n",
    "            em = compute_exact_match(normalized_prediction, normalized_ground_truths)\n",
    "            f1 = compute_f1(normalized_prediction, normalized_ground_truths)\n",
    "\n",
    "            predictions.append({\n",
    "                \"id\": example['id'],\n",
    "                \"prediction_text\": normalized_prediction\n",
    "            })\n",
    "\n",
    "            references.append({\n",
    "                \"id\": example['id'],\n",
    "                \"answers\": {\n",
    "                    \"text\": result['ground_truths'],\n",
    "                    \"answer_start\": example['answers']['answer_start']\n",
    "                }\n",
    "            })\n",
    "\n",
    "            per_example_results.append({\n",
    "                'id': example['id'],\n",
    "                'prediction_text': normalized_prediction,\n",
    "                'ground_truth_text': \"; \".join(normalized_ground_truths),\n",
    "                'answer_start': \"; \".join(map(str, example['answers']['answer_start'])),\n",
    "                'exact_match': em,\n",
    "                'f1_score': round(f1, 4)\n",
    "            })\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute overall metrics\n",
    "    results = squad_metric.compute(predictions=predictions, references=references)\n",
    "    elapsed_time = end - start\n",
    "\n",
    "    # Save per-example results to CSV\n",
    "    if save_path:\n",
    "        with open(save_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['id', 'prediction_text', 'ground_truth_text', 'answer_start', 'exact_match', 'f1_score']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in per_example_results:\n",
    "                writer.writerow(row)\n",
    "\n",
    "    return results, len(references), elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU offload supported: True\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "\n",
    "print(\"GPU offload supported:\", llama_cpp.llama_supports_gpu_offload())\n",
    "\n",
    "# Path to your GGUF model (adjust path if needed)\n",
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/llama-3.1-8B-Instruct-f16.gguf\"\n",
    "\n",
    "# Initialize Llama with GPU layers offloaded\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=-1,     # Offload to GPU! Adjust as needed based on your VRAM\n",
    "    verbose=False,         # Prints backend info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B-Instruct) float16 QA Results:\n",
      "\n",
      "Number of examples: 101\n",
      "Elapsed time: 300.17 seconds\n",
      "\n",
      "QA Evaluation Results:\n",
      "exact_match: 68.3168\n",
      "f1: 84.6438\n"
     ]
    }
   ],
   "source": [
    "results, num_examples, elapsed_time = evaluate_qa_with_llama_cpp(\n",
    "    sampled_questions,\n",
    "    qa_with_llama_cpp,\n",
    "    skip_unanswerable=True,\n",
    "    save_path=\"evaluation_with_predictions_and_references_f16.csv\"\n",
    ")\n",
    "\n",
    "# Print final report\n",
    "print(\"llama.cpp (Llama-3.1-8B-Instruct) float16 QA Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {num_examples}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nQA Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "llm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA L4, compute capability 8.9, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU offload supported: True\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "\n",
    "print(\"GPU offload supported:\", llama_cpp.llama_supports_gpu_offload())\n",
    "\n",
    "# Path to your GGUF model (adjust path if needed)\n",
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/llama-3.1-8B-Instruct-8bit.gguf\"\n",
    "\n",
    "# Initialize Llama with GPU layers offloaded\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=-1,     # Offload to GPU! Adjust as needed based on your VRAM\n",
    "    verbose=False         # Prints backend info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B-Instruct) 8Bit QA Results:\n",
      "\n",
      "Number of examples: 101\n",
      "Elapsed time: 178.62 seconds\n",
      "\n",
      "QA Evaluation Results:\n",
      "exact_match: 70.2970\n",
      "f1: 85.1146\n"
     ]
    }
   ],
   "source": [
    "results, num_examples, elapsed_time = evaluate_qa_with_llama_cpp(\n",
    "    sampled_questions,\n",
    "    qa_with_llama_cpp,\n",
    "    skip_unanswerable=True,\n",
    "    save_path=\"evaluation_with_predictions_and_references_8bit.csv\"\n",
    ")\n",
    "\n",
    "# Print final report\n",
    "print(\"llama.cpp (Llama-3.1-8B-Instruct) 8Bit QA Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {num_examples}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nQA Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "llm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU offload supported: True\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "\n",
    "print(\"GPU offload supported:\", llama_cpp.llama_supports_gpu_offload())\n",
    "\n",
    "# Path to your GGUF model (adjust path if needed)\n",
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n",
    "\n",
    "# Initialize Llama with GPU layers offloaded\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=-1,     # Offload to GPU! Adjust as needed based on your VRAM\n",
    "    verbose=False         # Prints backend info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B-Instruct) 4Bit QA Results:\n",
      "\n",
      "Number of examples: 101\n",
      "Elapsed time: 111.71 seconds\n",
      "\n",
      "QA Evaluation Results:\n",
      "exact_match: 67.3267\n",
      "f1: 84.0383\n"
     ]
    }
   ],
   "source": [
    "results, num_examples, elapsed_time = evaluate_qa_with_llama_cpp(\n",
    "    sampled_questions,\n",
    "    qa_with_llama_cpp,\n",
    "    skip_unanswerable=True,\n",
    "    save_path=\"evaluation_with_predictions_and_references_4bit.csv\"\n",
    ")\n",
    "\n",
    "# Print final report\n",
    "print(\"llama.cpp (Llama-3.1-8B-Instruct) 4Bit QA Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {num_examples}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nQA Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "llm.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 10:11:38.232516: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742033498.260125  423345 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742033498.268426  423345 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742033498.290468  423345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742033498.290492  423345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742033498.290495  423345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742033498.290498  423345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-15 10:11:38.297882: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from evaluate import load\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import time\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "\n",
    "# Load SQuAD v2 dataset (validation split)\n",
    "squad_v2 = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Convert the validation split to a list and sample 200 random questions\n",
    "validation_list = list(squad_v2[\"validation\"])\n",
    "sampled_questions = random.sample(validation_list, 200)\n",
    "\n",
    "questions_with_answers = [i for i in sampled_questions if len(i['answers']['text']) > 0]\n",
    "\n",
    "len(questions_with_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles, and extra whitespace.\"\"\"\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punctuation(lower(s))))\n",
    "\n",
    "def clean_prediction(prediction):\n",
    "    \"\"\"\n",
    "    Cleans the raw prediction output from llama.cpp.\n",
    "    - Truncates at a new line, 'Context:', or other stop signals.\n",
    "    - Normalizes the prediction.\n",
    "    \"\"\"\n",
    "    # Split on common stop sequences\n",
    "    stop_tokens = [\"\\n\\n\", \"\\nContext:\", \"Context:\", \"Question:\"]\n",
    "    for stop in stop_tokens:\n",
    "        if stop in prediction:\n",
    "            prediction = prediction.split(stop)[0]\n",
    "\n",
    "    return normalize_answer(prediction)\n",
    "\n",
    "\n",
    "def compute_exact_match(prediction, ground_truths):\n",
    "    \"\"\"Exact match: 1 if prediction is in ground_truths, else 0.\"\"\"\n",
    "    return int(prediction in ground_truths)\n",
    "\n",
    "def compute_f1(prediction, ground_truths):\n",
    "    \"\"\"Compute the maximum F1 over all ground truths.\"\"\"\n",
    "    def get_tokens(s):\n",
    "        return normalize_answer(s).split()\n",
    "\n",
    "    pred_tokens = get_tokens(prediction)\n",
    "    if not pred_tokens:\n",
    "        return int(not any(get_tokens(gt) for gt in ground_truths))\n",
    "\n",
    "    scores = []\n",
    "    for gt in ground_truths:\n",
    "        gt_tokens = get_tokens(gt)\n",
    "        common = set(pred_tokens) & set(gt_tokens)\n",
    "        num_same = len(common)\n",
    "\n",
    "        if num_same == 0:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "\n",
    "        precision = num_same / len(pred_tokens)\n",
    "        recall = num_same / len(gt_tokens)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        scores.append(f1)\n",
    "\n",
    "    return max(scores)\n",
    "\n",
    "def qa_prompt(example):\n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "\n",
    "    prompt_template = (\n",
    "        \"You are a question answering assistant. Given the context, answer the question. \"\n",
    "        \"If the answer isn't in the context, respond 'I don't know'.\\n\\n\"\n",
    "\n",
    "        \"Here is an example:\\n\"\n",
    "        \"Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni)...\\n\"\n",
    "        \"Question: What is the name of the region the Normans gave their name to?\\n\"\n",
    "        \"Answer: Normandy\\n\\n\"\n",
    "\n",
    "        \"Context: {context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    return prompt_template.format(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"llama-3.1-8B-Instruct-Q8_0\"\n",
    "\n",
    "llama_model_path = f\"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/{model_name}.gguf\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"llama.cpp\",\n",
    "    llama_model_path=llama_model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=10,\n",
    "    model_size= os.path.getsize(llama_model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "results = benchmark.benchmark([qa_prompt(i) for i in questions_with_answers])\n",
    "\n",
    "results[\"Generated Answer\"] = results[\"Generated Answer\"].apply(lambda x: clean_prediction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Length</th>\n",
       "      <th>Question (Prompt)</th>\n",
       "      <th>Generated Answer</th>\n",
       "      <th>FTL (s)</th>\n",
       "      <th>ATL (s)</th>\n",
       "      <th>GL (s)</th>\n",
       "      <th>TPS (tokens/s)</th>\n",
       "      <th>SPS (sentences/s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>KV-Cache Size Estimation (MB)</th>\n",
       "      <th>Total Energy (Wh)</th>\n",
       "      <th>Energy per Token (J/token)</th>\n",
       "      <th>Energy per Sentence (J/sentence)</th>\n",
       "      <th>Energy per Second (W)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1131</td>\n",
       "      <td>You are a question answering assistant. Given ...</td>\n",
       "      <td>lothar de maizi√®re</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>5.13</td>\n",
       "      <td>1.28</td>\n",
       "      <td>9122.06</td>\n",
       "      <td>8540.770976</td>\n",
       "      <td>581.289024</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>8.626822</td>\n",
       "      <td>34.507287</td>\n",
       "      <td>44.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>609</td>\n",
       "      <td>You are a question answering assistant. Given ...</td>\n",
       "      <td>some complexity classes</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.4227</td>\n",
       "      <td>18.93</td>\n",
       "      <td>2.37</td>\n",
       "      <td>9122.06</td>\n",
       "      <td>8540.770976</td>\n",
       "      <td>581.289024</td>\n",
       "      <td>0.007052</td>\n",
       "      <td>3.173491</td>\n",
       "      <td>25.387930</td>\n",
       "      <td>60.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prompt Length                                  Question (Prompt)  \\\n",
       "0           1131  You are a question answering assistant. Given ...   \n",
       "1            609  You are a question answering assistant. Given ...   \n",
       "\n",
       "          Generated Answer  FTL (s)  ATL (s)  GL (s)  TPS (tokens/s)  \\\n",
       "0       lothar de maizi√®re   0.1950   0.1950  0.7802            5.13   \n",
       "1  some complexity classes   0.0528   0.0528  0.4227           18.93   \n",
       "\n",
       "   SPS (sentences/s)  Memory Usage (MB)  Model Size (MB)  \\\n",
       "0               1.28            9122.06      8540.770976   \n",
       "1               2.37            9122.06      8540.770976   \n",
       "\n",
       "   KV-Cache Size Estimation (MB)  Total Energy (Wh)  \\\n",
       "0                     581.289024           0.009585   \n",
       "1                     581.289024           0.007052   \n",
       "\n",
       "   Energy per Token (J/token)  Energy per Sentence (J/sentence)  \\\n",
       "0                    8.626822                         34.507287   \n",
       "1                    3.173491                         25.387930   \n",
       "\n",
       "   Energy per Second (W)  \n",
       "0                  44.23  \n",
       "1                  60.06  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"Exact Match\"] = 0\n",
    "results[\"F1\"] = 0.0\n",
    "\n",
    "for i, j in enumerate(questions_with_answers):\n",
    "    pred = normalize_answer(results[\"Generated Answer\"].iloc[i])\n",
    "    gt_answers = [normalize_answer(ans) for ans in j[\"answers\"][\"text\"]]\n",
    "    \n",
    "    em = compute_exact_match(pred, gt_answers)\n",
    "    f1 = compute_f1(pred, gt_answers)\n",
    "    \n",
    "    results.loc[i, \"Exact Match\"] = em\n",
    "    results.loc[i, \"F1\"] = f1\n",
    "\n",
    "#safe results to csv\n",
    "\n",
    "results.to_csv(f\"results_cpp/{model_name}_Q&A\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A (squad_v2) LLama.cpp (Llama-3.1-8B-Instruct) Q8_0 Results:\n",
      "\n",
      "Number of examples: 101\n",
      "\n",
      "Mean Scores:\n",
      "\n",
      "Prompt Length: 1248.2970\n",
      "ATL (s): 0.0736\n",
      "GL (s): 0.5008\n",
      "TPS (tokens/s): 14.5372\n",
      "SPS (sentences/s): 2.6915\n",
      "Memory Usage (MB): 9135.4461\n",
      "Model Size (MB): 8540.7710\n",
      "KV-Cache Size Estimation (MB): 594.6752\n",
      "Total Energy (Wh): 0.0095\n",
      "Energy per Token (J/token): 5.0064\n",
      "Energy per Sentence (J/sentence): 29.2293\n",
      "Energy per Second (W): 68.5086\n",
      "Exact Match: 0.6733\n",
      "F1: 0.8498\n"
     ]
    }
   ],
   "source": [
    "# Select column 0 and columns 4 to the end\n",
    "first_column = results.iloc[:, [0]]\n",
    "remaining_columns = results.iloc[:, 4:]\n",
    "\n",
    "# Combine into a new DataFrame\n",
    "selected_columns = pd.concat([first_column, remaining_columns], axis=1)\n",
    "\n",
    "# Compute means for numeric columns only\n",
    "column_means = selected_columns.mean(numeric_only=True)\n",
    "\n",
    "# Display results\n",
    "print(\"Q&A (squad_v2) LLama.cpp (Llama-3.1-8B-Instruct) Q8_0 Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(questions_with_answers)}\")\n",
    "\n",
    "print(\"\\nMean Scores:\\n\")\n",
    "for column, mean_value in column_means.items():\n",
    "    print(f\"{column}: {mean_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA 3.1 8B Instruct - Quantization Benchmark (SQuAD v2 Q&A)\n",
    "\n",
    "<small>\n",
    "\n",
    "\n",
    "| **Metric**                        | **4-bit (Q4_K_M)**              | **8-bit (Q8_0)**              | **fp16**                      |\n",
    "|-----------------------------------|---------------------------------|-------------------------------|-------------------------------|\n",
    "| **Quantization Technique**        | 4-bit Group Quantization (Q4_K_M) | 8-bit Quantization (Q8_0)    | FP16 (Half-Precision Float)   |\n",
    "| **Number of Examples**            | 101                             | 101                           | 101                           |\n",
    "| **Prompt Length (avg)**           | 1248.2970 tokens                | 1248.2970 tokens              | 1248.2970 tokens              |\n",
    "| **ATL (Average Token Latency)**   | **0.0539 s/token**              | 0.0736 s/token                | 0.1077 s/token                |\n",
    "| **GL (Generation Latency)**       | **0.3762 s**                    | 0.5008 s                      | 0.7518 s                      |\n",
    "| **TPS (Tokens/sec)**              | **19.8857**                     | 14.5372                       | 9.7952                        |\n",
    "| **SPS (Sentences/sec)**           | **3.3143**                      | 2.6915                        | 1.7902                        |\n",
    "| **Memory Usage (MB)**             | **5933.5451 MB**                | 9135.4461 MB                  | 15912.1392 MB                 |\n",
    "| **Model Size (MB)**               | **4920.7344 MB**                | 8540.7710 MB                  | 16068.8913 MB                 |\n",
    "| **Total Energy (Wh)**             | **0.0072 Wh**                   | 0.0095 Wh                     | 0.0147 Wh                     |\n",
    "| **Energy per Token (J/token)**    | **3.6804**                      | 5.0064                        | 7.5682                        |\n",
    "| **Energy per Sentence (J/sentence)** | **22.9896**                   | 29.2293                       | 44.9827                       |\n",
    "| **Energy per Second (W)**         | **68.7679 W**                   | 68.5086 W                     | 70.4359 W                     |\n",
    "| **Exact Match (EM)**              | 0.6634                          | **0.6733**                    | 0.6634                        |\n",
    "| **F1 Score**                      | **0.8539**                      | 0.8498                        | 0.8399                        |\n",
    "\n",
    "\n",
    "</small>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
